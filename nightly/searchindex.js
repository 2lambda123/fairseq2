Search.setIndex({"docnames": ["bibliography", "index", "reference/abc", "reference/all", "reference/classes", "reference/data", "reference/enums", "reference/functions", "reference/generated/abc/fairseq2.gang.Gang", "reference/generated/classes/fairseq2.optim.lr_scheduler.CosineAnnealingLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.LRSchedulerBase", "reference/generated/classes/fairseq2.optim.lr_scheduler.MyleLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.NoamLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.PolynomialDecayLR", "reference/generated/data/fairseq2.data.ByteStreamError", "reference/generated/data/fairseq2.data.CString", "reference/generated/data/fairseq2.data.CollateOptionsOverride", "reference/generated/data/fairseq2.data.Collater", "reference/generated/data/fairseq2.data.DataPipeline", "reference/generated/data/fairseq2.data.DataPipelineBuilder", "reference/generated/data/fairseq2.data.DataPipelineError", "reference/generated/data/fairseq2.data.FileMapper", "reference/generated/data/fairseq2.data.PathLike", "reference/generated/data/fairseq2.data.RecordError", "reference/generated/data/fairseq2.data.StringLike", "reference/generated/data/fairseq2.data.VocabularyInfo", "reference/generated/data/fairseq2.data.get_last_failed_example", "reference/generated/data/fairseq2.data.is_string_like", "reference/generated/data/fairseq2.data.list_files", "reference/generated/data/fairseq2.data.read_sequence", "reference/generated/data/fairseq2.data.read_zipped_records", "reference/generated/data/fairseq2.data.text.read_text", "reference/generated/data_text/fairseq2.data.text.LineEnding", "reference/generated/data_text/fairseq2.data.text.MultilingualTextTokenizer", "reference/generated/data_text/fairseq2.data.text.SentencePieceDecoder", "reference/generated/data_text/fairseq2.data.text.SentencePieceEncoder", "reference/generated/data_text/fairseq2.data.text.SentencePieceModel", "reference/generated/data_text/fairseq2.data.text.StrSplitter", "reference/generated/data_text/fairseq2.data.text.StrToIntConverter", "reference/generated/data_text/fairseq2.data.text.StrToTensorConverter", "reference/generated/data_text/fairseq2.data.text.TextTokenDecoder", "reference/generated/data_text/fairseq2.data.text.TextTokenEncoder", "reference/generated/data_text/fairseq2.data.text.TextTokenizer", "reference/generated/data_text/fairseq2.data.text.vocabulary_from_sentencepiece", "reference/generated/enums/fairseq2.nn.transformer.TransformerNormOrder", "reference/generated/functions/fairseq2.nn.utils.mask.to_float_mask"], "filenames": ["bibliography.rst", "index.rst", "reference/abc.rst", "reference/all.rst", "reference/classes.rst", "reference/data.rst", "reference/enums.rst", "reference/functions.rst", "reference/generated/abc/fairseq2.gang.Gang.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.CosineAnnealingLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.LRSchedulerBase.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.MyleLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.NoamLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.PolynomialDecayLR.rst", "reference/generated/data/fairseq2.data.ByteStreamError.rst", "reference/generated/data/fairseq2.data.CString.rst", "reference/generated/data/fairseq2.data.CollateOptionsOverride.rst", "reference/generated/data/fairseq2.data.Collater.rst", "reference/generated/data/fairseq2.data.DataPipeline.rst", "reference/generated/data/fairseq2.data.DataPipelineBuilder.rst", "reference/generated/data/fairseq2.data.DataPipelineError.rst", "reference/generated/data/fairseq2.data.FileMapper.rst", "reference/generated/data/fairseq2.data.PathLike.rst", "reference/generated/data/fairseq2.data.RecordError.rst", "reference/generated/data/fairseq2.data.StringLike.rst", "reference/generated/data/fairseq2.data.VocabularyInfo.rst", "reference/generated/data/fairseq2.data.get_last_failed_example.rst", "reference/generated/data/fairseq2.data.is_string_like.rst", "reference/generated/data/fairseq2.data.list_files.rst", "reference/generated/data/fairseq2.data.read_sequence.rst", "reference/generated/data/fairseq2.data.read_zipped_records.rst", "reference/generated/data/fairseq2.data.text.read_text.rst", "reference/generated/data_text/fairseq2.data.text.LineEnding.rst", "reference/generated/data_text/fairseq2.data.text.MultilingualTextTokenizer.rst", "reference/generated/data_text/fairseq2.data.text.SentencePieceDecoder.rst", "reference/generated/data_text/fairseq2.data.text.SentencePieceEncoder.rst", "reference/generated/data_text/fairseq2.data.text.SentencePieceModel.rst", "reference/generated/data_text/fairseq2.data.text.StrSplitter.rst", "reference/generated/data_text/fairseq2.data.text.StrToIntConverter.rst", "reference/generated/data_text/fairseq2.data.text.StrToTensorConverter.rst", "reference/generated/data_text/fairseq2.data.text.TextTokenDecoder.rst", "reference/generated/data_text/fairseq2.data.text.TextTokenEncoder.rst", "reference/generated/data_text/fairseq2.data.text.TextTokenizer.rst", "reference/generated/data_text/fairseq2.data.text.vocabulary_from_sentencepiece.rst", "reference/generated/enums/fairseq2.nn.transformer.TransformerNormOrder.rst", "reference/generated/functions/fairseq2.nn.utils.mask.to_float_mask.rst"], "titles": ["Bibliography", "fairseq2 documentation", "ABCs and Protocols", "All", "Classes", "fairseq2.data", "Enums", "Functions", "Gang", "CosineAnnealingLR", "LRSchedulerBase", "MyleLR", "NoamLR", "PolynomialDecayLR", "fairseq2.data.ByteStreamError", "CString", "CollateOptionsOverride", "Collater", "DataPipeline", "DataPipelineBuilder", "fairseq2.data.DataPipelineError", "FileMapper", "PathLike", "fairseq2.data.RecordError", "StringLike", "VocabularyInfo", "get_last_failed_example", "is_string_like", "list_files", "read_sequence", "read_zipped_records", "read_text", "LineEnding", "MultilingualTextTokenizer", "SentencePieceDecoder", "SentencePieceEncoder", "SentencePieceModel", "StrSplitter", "StrToIntConverter", "StrToTensorConverter", "TextTokenDecoder", "TextTokenEncoder", "TextTokenizer", "vocabulary_from_sentencepiece", "TransformerNormOrder", "to_float_mask"], "terms": {"lh17": [0, 9], "ilya": 0, "loshchilov": [0, 9], "frank": 0, "hutter": [0, 9], "sgdr": 0, "stochast": 0, "gradient": 0, "descent": 0, "warm": 0, "restart": [0, 9], "2017": 0, "arxiv": 0, "1608": 0, "03983": 0, "swo21": [0, 44], "sam": 0, "shleifer": [0, 44], "jason": 0, "weston": 0, "myle": [0, 11], "ott": [0, 11], "normform": 0, "improv": 0, "transform": [0, 44], "pretrain": 0, "extra": 0, "normal": [0, 44], "2021": 0, "url": 0, "http": 0, "org": 0, "ab": 0, "2110": 0, "09456": 0, "doi": 0, "10": [0, 5, 19, 38], "48550": 0, "vsp": [0, 12, 44], "17": [0, 12, 44], "ashish": 0, "vaswani": [0, 12, 44], "noam": [0, 11, 12], "shazeer": [0, 12], "niki": 0, "parmar": 0, "jakob": 0, "uszkoreit": 0, "llion": 0, "jone": 0, "aidan": 0, "n": 0, "gomez": 0, "lukasz": 0, "kaiser": 0, "illia": 0, "polosukhin": 0, "attent": 0, "i": [0, 1, 5, 9, 10, 11, 12, 13, 17, 18, 19, 21, 23, 25, 27, 33, 35, 41], "all": [0, 1, 8, 9, 11, 13, 16, 17, 18, 19, 21, 28, 37], "you": [0, 5, 21], "need": [0, 17], "1706": 0, "03762": 0, "xyh": [0, 44], "20": [0, 44], "ruibin": 0, "xiong": [0, 44], "yunchang": 0, "yang": 0, "di": 0, "he": 0, "kai": 0, "zheng": 0, "shuxin": 0, "chen": 0, "xing": 0, "huishuai": 0, "zhang": 0, "yanyan": 0, "lan": 0, "liwei": 0, "wang": 0, "tie": 0, "yan": 0, "liu": 0, "On": 0, "layer": [0, 44], "architectur": 0, "2020": 0, "2002": 0, "04745": 0, "sequenc": [1, 9, 11, 13, 17, 18, 19, 25, 29, 37], "model": [1, 12, 33, 34, 35, 43], "toolkit": 1, "allow": [1, 17, 18], "research": 1, "develop": 1, "train": [1, 9, 11, 12, 13], "custom": 1, "translat": [1, 42], "summar": 1, "languag": [1, 33, 42], "other": [1, 5], "content": [1, 21], "gener": [1, 33, 42], "task": [1, 33, 42], "data": [1, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 45], "bibliographi": 1, "provid": 5, "python": [5, 15, 18, 21], "build": 5, "c": 5, "datapipelin": [5, 19], "The": [5, 8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 21, 25, 28, 29, 33, 34, 35, 37, 40, 41, 42, 45], "dataload": [5, 18, 37], "abl": 5, "leverag": 5, "sever": [5, 18, 19, 21], "thread": 5, "work": [5, 8], "around": [5, 18], "global": 5, "interpret": 5, "lock": 5, "limit": 5, "also": [5, 9, 18, 21], "better": 5, "perform": 5, "than": [5, 19], "pure": 5, "look": [5, 21], "like": 5, "thi": [5, 8, 9, 11, 12, 13, 15, 16, 18, 19, 21], "read_text": [5, 37], "file": [5, 14, 21, 28, 30, 31, 33, 37], "tsv": [5, 37], "map": [5, 18, 19, 21, 37], "lambda": [5, 19, 37], "x": [5, 19, 37], "str": [5, 15, 16, 18, 19, 21, 22, 24, 27, 28, 33, 34, 35, 37, 40, 41, 42], "split": [5, 15, 37], "t": [5, 9, 11, 12, 13, 14, 37], "1": [5, 9, 10, 11, 12, 13, 16, 17, 19, 32, 35, 37, 44], "lower": 5, "filter": [5, 19], "len": 5, "function": [5, 19, 37, 38, 39], "item": 5, "go": [5, 37], "through": 5, "pipelin": [5, 18, 19, 20, 31], "don": 5, "have": [5, 17, 18], "flat": 5, "tensor": [5, 8, 17, 33, 34, 35, 39, 40, 41, 42, 45], "can": [5, 14, 18, 19, 21], "tupl": [5, 17], "dictionari": [5, 17, 18, 37], "oper": [5, 8, 18], "specifi": [5, 16, 19, 21, 33, 44], "specif": [5, 17, 42], "input": [5, 8, 17, 19, 37], "notabl": 5, "datapipelinebuild": [5, 18, 28, 29, 30, 31], "ha": [5, 9, 19], "selector": [5, 16, 19], "argument": [5, 42], "choos": 5, "appli": [5, 16, 19, 44], "If": [5, 9, 11, 12, 13, 17, 18, 19, 28, 33, 42, 45], "3": [5, 12, 17, 19], "select": [5, 19], "third": 5, "foo": 5, "valu": [5, 9, 17, 32, 44], "correspond": [5, 11, 12, 13, 37], "kei": [5, 17, 18, 21], "nest": 5, "separ": [5, 19], "follow": [5, 17, 21, 28], "For": [5, 17, 21], "y": 5, "2": [5, 9, 17, 19, 37, 44], "4": [5, 17], "z": 5, "5": [5, 12, 17, 19], "bar": 5, "6": 5, "refer": [5, 9, 12, 42], "accept": 5, "them": [5, 8, 19], "comma": 5, "list": [5, 15, 17, 19, 28, 33, 34, 37, 40], "exampl": [5, 17, 18, 19, 37], "multipli": 5, "leav": 5, "unmodifi": 5, "helper": 5, "method": 5, "tool": 5, "token": [5, 25, 33, 34, 35, 40, 41, 42], "convert": [5, 37, 45], "from": [5, 8, 9, 10, 11, 12, 13, 17, 18, 19, 20, 33, 34, 40], "byte": [5, 15, 21], "class": [8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 25, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44], "fairseq2": [8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 22, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "rank": 8, "size": [8, 19, 25, 39], "devic": [8, 33, 35, 42], "sourc": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "base": [8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 25, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44], "abc": [8, 10, 40, 41, 42], "repres": [8, 9, 10, 11, 12, 13, 15, 25, 33, 42], "set": [8, 33], "process": [8, 19], "collect": 8, "paramet": [8, 9, 11, 12, 13, 15, 16, 17, 18, 19, 21, 28, 29, 33, 34, 35, 37, 40, 41, 42, 45], "int": [8, 9, 11, 12, 13, 17, 19, 21, 25, 37, 38], "number": [8, 9, 11, 12, 13, 19, 35, 41], "ar": [8, 42], "part": [8, 19], "associ": [8, 9, 11, 12, 13, 42], "abstract": [8, 10, 40, 41, 42], "all_gath": 8, "output_tensor": 8, "input_tensor": 8, "gather": 8, "put": 8, "singl": [8, 17, 19], "output": 8, "accomod": 8, "element": [8, 25, 29], "current": [8, 9, 10, 11, 12, 13, 18, 19], "all_reduc": 8, "op": 8, "reduc": 8, "across": 8, "reduceoper": 8, "wise": 8, "as_process_group": 8, "return": [8, 9, 10, 11, 12, 13, 15, 17, 18, 19, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45], "group": [8, 9, 10, 11, 12, 13], "type": [8, 9, 15, 17, 18, 19, 21, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 37, 38, 39, 40, 41, 42, 43, 44, 45], "processgroup": 8, "barrier": 8, "synchron": 8, "final": [9, 11, 12, 13, 33, 34, 35, 36], "optim": [9, 10, 11, 12, 13], "lr_schedul": [9, 10, 11, 12, 13], "cycle_len": 9, "num_warmup_step": [9, 11, 12, 13], "cycle_mul": 9, "0": [9, 11, 13, 17, 19, 35, 37, 44], "lr_mul": 9, "start_lr": [9, 11, 13], "final_lr": [9, 13], "last_epoch": [9, 10, 11, 12, 13], "verbos": [9, 10, 11, 12, 13], "fals": [9, 10, 11, 12, 13, 17, 18, 19, 31, 33, 34, 35, 37, 42], "lrschedulerbas": [9, 11, 12, 13], "learn": [9, 10, 11, 12, 13], "rate": [9, 10, 11, 12, 13], "schedul": [9, 10, 11, 12, 13], "describ": [9, 12, 17, 25, 44], "dure": [9, 13], "warmup": [9, 11, 12, 13], "eta_t": [9, 11, 12, 13], "eta_": [9, 11, 12, 13], "frac": [9, 11, 12, 13], "t_": [9, 11, 12, 13], "after": [9, 12, 13, 44], "text": [9, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43], "co": 9, "pi": 9, "where": [9, 35, 41], "anneal": 9, "cycl": 9, "t_i": 9, "step": [9, 11, 12, 13], "taken": 9, "sinc": 9, "last": [9, 10, 11, 12, 13, 17, 19, 21], "total": [9, 13], "within": 9, "th": 9, "e": 9, "length": [9, 17], "cosin": 9, "effect": 9, "start": [9, 32, 44], "larg": [9, 19], "rel": [9, 21], "rapidli": 9, "decreas": [9, 11, 12, 13], "minimum": 9, "befor": 9, "being": [9, 19], "increas": [9, 11, 12, 13, 19], "again": 9, "pleas": 9, "paper": [9, 12], "more": [9, 19, 42], "about": 9, "detail": [9, 16, 19], "In": [9, 12], "addit": 9, "origin": [9, 11, 17], "implement": [9, 11, 42], "support": [9, 15, 33], "phase": 9, "linearli": [9, 11, 12, 13], "first": [9, 11, 12, 13, 18, 37], "chainabl": [9, 11, 12, 13], "float": [9, 11, 13, 18, 45], "factor": 9, "grow": 9, "each": [9, 11, 12, 13, 17, 18, 19, 30, 44], "scale": [9, 11], "end": [9, 15, 18, 25], "initi": [9, 11, 13], "respect": [9, 11, 13], "index": [9, 11, 12, 13, 19, 25], "epoch": [9, 10, 11, 12, 13], "bool": [9, 11, 12, 13, 18, 19, 33, 42], "true": [9, 11, 12, 13, 17, 18, 19, 27, 33, 42], "print": [9, 11, 12, 13], "messag": [9, 11, 12, 13], "stdout": [9, 11, 12, 13], "updat": [9, 11, 12, 13], "get_last_lr": [9, 10, 11, 12, 13], "comput": [9, 10, 11, 12, 13], "load_state_dict": [9, 10, 11, 12, 13, 18], "state_dict": [9, 10, 11, 12, 13, 18, 19], "load": [9, 10, 11, 12, 13, 19], "state": [9, 10, 11, 12, 13, 18, 19], "arg": [9, 10, 11, 12, 13], "dict": [9, 10, 11, 12, 13, 17, 18, 21, 37], "should": [9, 10, 11, 12, 13, 16], "an": [9, 10, 11, 12, 13, 15, 18, 20, 21, 25], "object": [9, 10, 11, 12, 13, 15, 16, 17, 19, 21, 25, 36, 37, 38, 39], "call": [9, 10, 11, 12, 13, 18, 19, 37, 38, 39], "print_lr": [9, 10, 11, 12, 13], "is_verbos": [9, 10, 11, 12, 13], "lr": [9, 10, 11, 12, 13], "none": [9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 21, 25, 28, 31, 32, 33, 35, 36, 37, 39, 41, 42, 44, 45], "displai": [9, 10, 11, 12, 13], "It": [9, 10, 11, 12, 13, 18, 33], "contain": [9, 10, 11, 12, 13, 18], "entri": [9, 10, 11, 12, 13, 37], "everi": [9, 10, 11, 12, 13, 19, 29], "variabl": [9, 10, 11, 12, 13], "self": [9, 10, 11, 12, 13, 19, 37, 38, 39], "__dict__": [9, 10, 11, 12, 13], "which": [9, 10, 11, 12, 13, 33, 42], "_lrschedul": 10, "version": [11, 19], "noamlr": 11, "preserv": [11, 17], "min": [11, 12], "sqrt": [11, 12], "essenti": 11, "squar": [11, 12], "root": [11, 12, 21], "wa": [11, 17], "propos": 11, "fairseq": 11, "under": [11, 28], "name": [11, 18, 21, 32, 37, 44], "inversesquarerootlr": 11, "thereaft": [11, 12, 13], "proportion": [11, 12], "invers": [11, 12], "section": 12, "et": [12, 44], "al": [12, 44], "author": 12, "us": [12, 13, 15, 16, 17, 18, 19, 21, 25, 33, 42, 45], "dimension": 12, "commonli": 12, "second": [12, 37], "num_step": 13, "power": 13, "polynomi": 13, "decai": 13, "p": 13, "degre": 13, "includ": 13, "over": [13, 18], "expon": 13, "except": [14, 20, 23], "rais": [14, 18, 20, 23], "when": [14, 17, 18, 20, 23], "dataset": [14, 23], "read": [14, 18, 19, 20, 21, 23, 29, 30, 31, 37], "": [15, 18, 19, 27, 35, 37, 38, 39, 41, 44], "immut": 15, "utf": 15, "8": [15, 19], "string": [15, 37], "zero": 15, "copi": 15, "marshal": 15, "between": [15, 42], "nativ": [15, 18], "code": 15, "lstrip": 15, "whitespac": 15, "begin": [15, 25, 44], "rstrip": 15, "sep": [15, 37], "word": 15, "delimit": 15, "pad_idx": [16, 17, 19, 25], "pad_to_multipl": [16, 17, 19], "overrid": [16, 17, 19], "how": 16, "collat": [16, 19], "creat": [16, 17, 18, 19, 33, 37, 42], "batch": [16, 17, 19], "particular": 16, "column": [16, 17, 19, 37], "same": [16, 17, 18, 19, 21], "pad": [16, 17, 25], "idx": 16, "multipl": [16, 17], "see": [16, 19], "syntax": [16, 19, 28], "concaten": [17, 18, 19], "dimens": 17, "otherwis": 17, "requir": 17, "made": 17, "long": 17, "enough": 17, "fit": 17, "longest": 17, "round": [17, 18], "up": [17, 21], "is_rag": 17, "seq": [17, 29], "seq_len": 17, "A": [17, 18, 21, 33], "shape": [17, 19, 35, 41, 45], "differ": [17, 42], "shortest": 17, "alwai": 17, "collateoptionsoverrid": 17, "__call__": [17, 21, 34, 35, 37, 38, 39, 40, 41], "ani": [17, 18, 19, 21, 26, 29, 45], "iter": 18, "persist": 18, "disk": 18, "resum": 18, "later": 18, "twice": 18, "two": 18, "share": 18, "so": [18, 21], "behav": 18, "inconcist": 18, "__iter__": [18, 32, 44], "modifi": 18, "intern": 18, "safe": 18, "static": 18, "concat": 18, "strict": [18, 19], "restor": [18, 19], "previous": 18, "enforc": [18, 21], "match": [18, 19, 28, 33], "reset": 18, "move": 18, "back": [18, 33], "round_robin": 18, "stop_at_shortest": 18, "extract": 18, "robin": 18, "stop": 18, "reach": 18, "its": 18, "circl": 18, "finish": 18, "until": 18, "sampl": [18, 19], "weight": 18, "data_pipelin": 18, "desir": 18, "distribut": 18, "uniform": 18, "posit": 18, "pass": 18, "zip": [18, 30], "zip_to_shortest": 18, "flatten": 18, "disable_parallel": 18, "togeth": 18, "assign": 18, "sequenti": 18, "properti": [18, 35, 41], "is_broken": 18, "broken": 18, "futur": 18, "datapipelineerror": 18, "api": 19, "and_return": [19, 37], "max_num_warn": 19, "new": 19, "instanc": 19, "bucket": 19, "bucket_s": 19, "drop_remaind": 19, "combin": 19, "consecut": 19, "drop": 19, "case": 19, "fewer": 19, "bucket_by_length": 19, "similar": 19, "equival": 19, "predic": 19, "keep": [19, 37], "onli": [19, 37], "those": 19, "who": 19, "callabl": 19, "fn": 19, "num_parallel_cal": 19, "usag": [19, 37], "yield": 19, "12": 19, "15": 19, "result": 19, "core": 19, "b": 19, "11": 19, "13": 19, "thei": 19, "automat": 19, "chain": 19, "f1": 19, "f2": 19, "effici": 19, "colum": 19, "parallel": 19, "prefetch": 19, "num_exampl": 19, "background": 19, "while": [19, 20, 23, 33, 42], "shard": 19, "shard_idx": 19, "num_shard": 19, "shuffl": 19, "shuffle_window": 19, "enabl": [19, 21], "fix": 19, "buffer": 19, "intermedi": 19, "randomli": 19, "replac": 19, "memori": [19, 21, 33, 42], "full": 19, "save": 19, "ensur": 19, "preemption": 19, "lost": 19, "significantli": 19, "time": 19, "disabl": 19, "skip": 19, "take": 19, "most": 19, "yield_from": 19, "error": 20, "occur": 20, "root_dir": 21, "cached_fd_count": 21, "given": [21, 37, 38], "slice": 21, "big_fil": 21, "txt": 21, "1024": 21, "48": 21, "offset": 21, "cstring": [21, 22, 24, 27, 28, 33, 34, 35, 37, 40, 41], "o": [21, 22, 28, 33], "pathlik": [21, 28, 33], "directori": 21, "warn": 21, "happili": 21, "system": 21, "lru": 21, "cach": 21, "especi": 21, "filenam": 21, "pars": [21, 38], "path": [21, 28], "memoryblock": 21, "block": 21, "get": [21, 35, 41], "regular": 21, "filemapperoutput": 21, "alia": [22, 24], "union": [22, 24], "corrupt": 23, "record": 23, "encount": 23, "unk_idx": 25, "bos_idx": 25, "eos_idx": 25, "vocabulari": [25, 42, 43], "symbol": 25, "unknown": 25, "typeguard": 27, "pathnam": [28, 30, 31, 33, 36], "pattern": 28, "recurs": 28, "travers": 28, "non": 28, "empti": 28, "fnmatch": 28, "archiv": 30, "encod": [31, 33, 35, 41, 42], "line_end": 31, "lineend": 31, "infer": 31, "ltrim": 31, "rtrim": 31, "skip_empti": 31, "memory_map": 31, "block_siz": 31, "open": 31, "line": 31, "one": [31, 37], "modul": [32, 44], "qualnam": [32, 44], "boundari": [32, 44], "enum": [32, 44], "classmethod": [32, 44], "member": [32, 44], "definit": [32, 44], "order": [32, 44], "source_lang": 33, "target_lang": 33, "default_source_lang": 33, "default_target_lang": 33, "texttoken": 33, "bilingu": 33, "multilingu": [33, 42], "sentencepiec": 33, "user": 33, "defin": 33, "valid": [33, 42], "create_encod": [33, 42], "target": [33, 42], "fall": 33, "create_decod": [33, 42], "decod": [33, 34, 40, 42], "texttokendecod": [33, 34, 42], "lang": [33, 42], "mode": [33, 42], "pin_memori": [33, 35, 42], "must": 33, "default": [33, 37, 45], "either": 33, "depend": 33, "construct": [33, 42], "pin": [33, 42], "texttokenencod": [33, 35, 42], "revers": [34, 35], "token_indic": [34, 40], "indic": [34, 35, 37, 40, 41, 42], "prefix_token": 35, "suffix_token": 35, "enable_sampl": 35, "nbest_siz": 35, "alpha": 35, "sentenc": [35, 40, 41, 42], "prefix_indic": [35, 41], "prefix": [35, 41], "suffix_indic": [35, 41], "suffix": [35, 41], "control_symbol": 36, "exclud": 37, "charact": 37, "tab": 37, "Will": 37, "per": 37, "va": 37, "cc": 37, "BY": 37, "franc": 37, "tatoeba": 37, "en": [37, 42], "fr": 37, "integ": 38, "dtype": [39, 45], "vocab_info": 42, "vocabularyinfo": [42, 43], "inform": [42, 43], "concret": 42, "subclass": 42, "typic": 42, "multi": 42, "job": 42, "distinguish": 42, "transcript": 42, "u": 42, "de": 42, "nn": [44, 45], "post": 44, "residu": 44, "connect": 44, "pre": 44, "pre_with_normform": 44, "util": 45, "mask": 45, "boolean": 45, "point": 45}, "objects": {"fairseq2.data": [[14, 0, 1, "", "ByteStreamError"], [15, 1, 1, "", "CString"], [16, 1, 1, "", "CollateOptionsOverride"], [17, 1, 1, "", "Collater"], [18, 1, 1, "", "DataPipeline"], [19, 1, 1, "", "DataPipelineBuilder"], [20, 0, 1, "", "DataPipelineError"], [21, 1, 1, "", "FileMapper"], [22, 4, 1, "", "PathLike"], [23, 0, 1, "", "RecordError"], [24, 4, 1, "", "StringLike"], [25, 1, 1, "", "VocabularyInfo"], [26, 6, 1, "", "get_last_failed_example"], [27, 6, 1, "", "is_string_like"], [28, 6, 1, "", "list_files"], [29, 6, 1, "", "read_sequence"], [30, 6, 1, "", "read_zipped_records"]], "fairseq2.data.CString": [[15, 2, 1, "", "bytes"], [15, 2, 1, "", "lstrip"], [15, 2, 1, "", "rstrip"], [15, 2, 1, "", "split"]], "fairseq2.data.Collater": [[17, 2, 1, "", "__call__"]], "fairseq2.data.DataPipeline": [[18, 2, 1, "", "__iter__"], [18, 2, 1, "", "concat"], [18, 3, 1, "", "is_broken"], [18, 2, 1, "", "load_state_dict"], [18, 2, 1, "", "reset"], [18, 2, 1, "", "round_robin"], [18, 2, 1, "", "sample"], [18, 2, 1, "", "state_dict"], [18, 2, 1, "", "zip"]], "fairseq2.data.DataPipelineBuilder": [[19, 2, 1, "", "and_return"], [19, 2, 1, "", "bucket"], [19, 2, 1, "", "bucket_by_length"], [19, 2, 1, "", "collate"], [19, 2, 1, "", "filter"], [19, 2, 1, "", "map"], [19, 2, 1, "", "prefetch"], [19, 2, 1, "", "shard"], [19, 2, 1, "", "shuffle"], [19, 2, 1, "", "skip"], [19, 2, 1, "", "take"], [19, 2, 1, "", "yield_from"]], "fairseq2.data.FileMapper": [[21, 2, 1, "", "__call__"]], "fairseq2.data.VocabularyInfo": [[25, 5, 1, "", "bos_idx"], [25, 5, 1, "", "eos_idx"], [25, 5, 1, "", "pad_idx"], [25, 5, 1, "", "size"], [25, 5, 1, "", "unk_idx"]], "fairseq2.data.text": [[32, 1, 1, "", "LineEnding"], [33, 1, 1, "", "MultilingualTextTokenizer"], [34, 1, 1, "", "SentencePieceDecoder"], [35, 1, 1, "", "SentencePieceEncoder"], [36, 1, 1, "", "SentencePieceModel"], [37, 1, 1, "", "StrSplitter"], [38, 1, 1, "", "StrToIntConverter"], [39, 1, 1, "", "StrToTensorConverter"], [40, 1, 1, "", "TextTokenDecoder"], [41, 1, 1, "", "TextTokenEncoder"], [42, 1, 1, "", "TextTokenizer"], [31, 6, 1, "", "read_text"], [43, 6, 1, "", "vocabulary_from_sentencepiece"]], "fairseq2.data.text.LineEnding": [[32, 2, 1, "", "__iter__"]], "fairseq2.data.text.MultilingualTextTokenizer": [[33, 2, 1, "", "create_decoder"], [33, 2, 1, "", "create_encoder"]], "fairseq2.data.text.SentencePieceDecoder": [[34, 2, 1, "", "__call__"]], "fairseq2.data.text.SentencePieceEncoder": [[35, 2, 1, "", "__call__"], [35, 3, 1, "", "prefix_indices"], [35, 3, 1, "", "suffix_indices"]], "fairseq2.data.text.StrSplitter": [[37, 2, 1, "", "__call__"]], "fairseq2.data.text.StrToIntConverter": [[38, 2, 1, "", "__call__"]], "fairseq2.data.text.StrToTensorConverter": [[39, 2, 1, "", "__call__"]], "fairseq2.data.text.TextTokenDecoder": [[40, 2, 1, "", "__call__"]], "fairseq2.data.text.TextTokenEncoder": [[41, 2, 1, "", "__call__"], [41, 3, 1, "", "prefix_indices"], [41, 3, 1, "", "suffix_indices"]], "fairseq2.data.text.TextTokenizer": [[42, 2, 1, "", "create_decoder"], [42, 2, 1, "", "create_encoder"]], "fairseq2.gang": [[8, 1, 1, "", "Gang"]], "fairseq2.gang.Gang": [[8, 2, 1, "", "all_gather"], [8, 2, 1, "", "all_reduce"], [8, 2, 1, "", "as_process_group"], [8, 2, 1, "", "barrier"]], "fairseq2.nn.transformer": [[44, 1, 1, "", "TransformerNormOrder"]], "fairseq2.nn.transformer.TransformerNormOrder": [[44, 5, 1, "", "POST"], [44, 5, 1, "", "PRE"], [44, 5, 1, "", "PRE_WITH_NORMFORMER"], [44, 2, 1, "", "__iter__"]], "fairseq2.nn.utils.mask": [[45, 6, 1, "", "to_float_mask"]], "fairseq2.optim.lr_scheduler": [[9, 1, 1, "", "CosineAnnealingLR"], [10, 1, 1, "", "LRSchedulerBase"], [11, 1, 1, "", "MyleLR"], [12, 1, 1, "", "NoamLR"], [13, 1, 1, "", "PolynomialDecayLR"]], "fairseq2.optim.lr_scheduler.CosineAnnealingLR": [[9, 2, 1, "", "get_last_lr"], [9, 2, 1, "", "load_state_dict"], [9, 2, 1, "", "print_lr"], [9, 2, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.LRSchedulerBase": [[10, 2, 1, "", "get_last_lr"], [10, 2, 1, "", "load_state_dict"], [10, 2, 1, "", "print_lr"], [10, 2, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.MyleLR": [[11, 2, 1, "", "get_last_lr"], [11, 2, 1, "", "load_state_dict"], [11, 2, 1, "", "print_lr"], [11, 2, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.NoamLR": [[12, 2, 1, "", "get_last_lr"], [12, 2, 1, "", "load_state_dict"], [12, 2, 1, "", "print_lr"], [12, 2, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.PolynomialDecayLR": [[13, 2, 1, "", "get_last_lr"], [13, 2, 1, "", "load_state_dict"], [13, 2, 1, "", "print_lr"], [13, 2, 1, "", "state_dict"]]}, "objtypes": {"0": "py:exception", "1": "py:class", "2": "py:method", "3": "py:property", "4": "py:data", "5": "py:attribute", "6": "py:function"}, "objnames": {"0": ["py", "exception", "Python exception"], "1": ["py", "class", "Python class"], "2": ["py", "method", "Python method"], "3": ["py", "property", "Python property"], "4": ["py", "data", "Python data"], "5": ["py", "attribute", "Python attribute"], "6": ["py", "function", "Python function"]}, "titleterms": {"bibliographi": 0, "fairseq2": [1, 5, 14, 20, 23], "document": 1, "refer": 1, "misc": 1, "abc": [2, 3], "protocol": [2, 3], "all": 3, "class": [3, 4, 5], "enum": [3, 6], "function": [3, 7], "data": [5, 14, 20, 23], "column": 5, "syntax": 5, "public": 5, "us": 5, "api": 5, "text": 5, "gang": 8, "cosineannealinglr": 9, "lrschedulerbas": 10, "mylelr": 11, "noamlr": 12, "polynomialdecaylr": 13, "bytestreamerror": 14, "cstring": 15, "collateoptionsoverrid": 16, "collat": 17, "datapipelin": 18, "datapipelinebuild": 19, "datapipelineerror": 20, "filemapp": 21, "pathlik": 22, "recorderror": 23, "stringlik": 24, "vocabularyinfo": 25, "get_last_failed_exampl": 26, "is_string_lik": 27, "list_fil": 28, "read_sequ": 29, "read_zipped_record": 30, "read_text": 31, "lineend": 32, "multilingualtexttoken": 33, "sentencepiecedecod": 34, "sentencepieceencod": 35, "sentencepiecemodel": 36, "strsplitter": 37, "strtointconvert": 38, "strtotensorconvert": 39, "texttokendecod": 40, "texttokenencod": 41, "texttoken": 42, "vocabulary_from_sentencepiec": 43, "transformernormord": 44, "to_float_mask": 45}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinxcontrib.bibtex": 9, "sphinx": 57}, "alltitles": {"Bibliography": [[0, "bibliography"]], "fairseq2 documentation": [[1, "fairseq2-documentation"]], "fairseq2 Reference": [[1, null]], "Misc": [[1, null]], "ABCs and Protocols": [[2, "abcs-and-protocols"], [3, "abcs-and-protocols"]], "All": [[3, "all"]], "Classes": [[3, "classes"], [4, "classes"]], "Enums": [[3, "enums"], [6, "enums"]], "Functions": [[3, "functions"], [7, "functions"]], "fairseq2.data": [[5, "fairseq2-data"]], "Column syntax": [[5, "column-syntax"]], "Public classes used in fairseq2 API:": [[5, "public-classes-used-in-fairseq2-api"]], "fairseq2.data.text": [[5, "fairseq2-data-text"]], "Gang": [[8, "gang"]], "CosineAnnealingLR": [[9, "cosineannealinglr"]], "LRSchedulerBase": [[10, "lrschedulerbase"]], "MyleLR": [[11, "mylelr"]], "NoamLR": [[12, "noamlr"]], "PolynomialDecayLR": [[13, "polynomialdecaylr"]], "fairseq2.data.ByteStreamError": [[14, "fairseq2-data-bytestreamerror"]], "CString": [[15, "cstring"]], "CollateOptionsOverride": [[16, "collateoptionsoverride"]], "Collater": [[17, "collater"]], "DataPipeline": [[18, "datapipeline"]], "DataPipelineBuilder": [[19, "datapipelinebuilder"]], "fairseq2.data.DataPipelineError": [[20, "fairseq2-data-datapipelineerror"]], "FileMapper": [[21, "filemapper"]], "PathLike": [[22, "pathlike"]], "fairseq2.data.RecordError": [[23, "fairseq2-data-recorderror"]], "StringLike": [[24, "stringlike"]], "VocabularyInfo": [[25, "vocabularyinfo"]], "get_last_failed_example": [[26, "get-last-failed-example"]], "is_string_like": [[27, "is-string-like"]], "list_files": [[28, "list-files"]], "read_sequence": [[29, "read-sequence"]], "read_zipped_records": [[30, "read-zipped-records"]], "read_text": [[31, "read-text"]], "LineEnding": [[32, "lineending"]], "MultilingualTextTokenizer": [[33, "multilingualtexttokenizer"]], "SentencePieceDecoder": [[34, "sentencepiecedecoder"]], "SentencePieceEncoder": [[35, "sentencepieceencoder"]], "SentencePieceModel": [[36, "sentencepiecemodel"]], "StrSplitter": [[37, "strsplitter"]], "StrToIntConverter": [[38, "strtointconverter"]], "StrToTensorConverter": [[39, "strtotensorconverter"]], "TextTokenDecoder": [[40, "texttokendecoder"]], "TextTokenEncoder": [[41, "texttokenencoder"]], "TextTokenizer": [[42, "texttokenizer"]], "vocabulary_from_sentencepiece": [[43, "vocabulary-from-sentencepiece"]], "TransformerNormOrder": [[44, "transformernormorder"]], "to_float_mask": [[45, "to-float-mask"]]}, "indexentries": {"gang (class in fairseq2.gang)": [[8, "fairseq2.gang.Gang"]], "all_gather() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.all_gather"]], "all_reduce() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.all_reduce"]], "as_process_group() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.as_process_group"]], "barrier() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.barrier"]], "cosineannealinglr (class in fairseq2.optim.lr_scheduler)": [[9, "fairseq2.optim.lr_scheduler.CosineAnnealingLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[9, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[9, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[9, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[9, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.state_dict"]], "lrschedulerbase (class in fairseq2.optim.lr_scheduler)": [[10, "fairseq2.optim.lr_scheduler.LRSchedulerBase"]], "get_last_lr() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[10, "fairseq2.optim.lr_scheduler.LRSchedulerBase.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[10, "fairseq2.optim.lr_scheduler.LRSchedulerBase.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[10, "fairseq2.optim.lr_scheduler.LRSchedulerBase.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[10, "fairseq2.optim.lr_scheduler.LRSchedulerBase.state_dict"]], "mylelr (class in fairseq2.optim.lr_scheduler)": [[11, "fairseq2.optim.lr_scheduler.MyleLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.mylelr method)": [[11, "fairseq2.optim.lr_scheduler.MyleLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.mylelr method)": [[11, "fairseq2.optim.lr_scheduler.MyleLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.mylelr method)": [[11, "fairseq2.optim.lr_scheduler.MyleLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.mylelr method)": [[11, "fairseq2.optim.lr_scheduler.MyleLR.state_dict"]], "noamlr (class in fairseq2.optim.lr_scheduler)": [[12, "fairseq2.optim.lr_scheduler.NoamLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.noamlr method)": [[12, "fairseq2.optim.lr_scheduler.NoamLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.noamlr method)": [[12, "fairseq2.optim.lr_scheduler.NoamLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.noamlr method)": [[12, "fairseq2.optim.lr_scheduler.NoamLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.noamlr method)": [[12, "fairseq2.optim.lr_scheduler.NoamLR.state_dict"]], "polynomialdecaylr (class in fairseq2.optim.lr_scheduler)": [[13, "fairseq2.optim.lr_scheduler.PolynomialDecayLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[13, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[13, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[13, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[13, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.state_dict"]], "bytestreamerror": [[14, "fairseq2.data.ByteStreamError"]], "cstring (class in fairseq2.data)": [[15, "fairseq2.data.CString"]], "bytes() (fairseq2.data.cstring method)": [[15, "fairseq2.data.CString.bytes"]], "lstrip() (fairseq2.data.cstring method)": [[15, "fairseq2.data.CString.lstrip"]], "rstrip() (fairseq2.data.cstring method)": [[15, "fairseq2.data.CString.rstrip"]], "split() (fairseq2.data.cstring method)": [[15, "fairseq2.data.CString.split"]], "collateoptionsoverride (class in fairseq2.data)": [[16, "fairseq2.data.CollateOptionsOverride"]], "collater (class in fairseq2.data)": [[17, "fairseq2.data.Collater"]], "__call__() (fairseq2.data.collater method)": [[17, "fairseq2.data.Collater.__call__"]], "datapipeline (class in fairseq2.data)": [[18, "fairseq2.data.DataPipeline"]], "__iter__() (fairseq2.data.datapipeline method)": [[18, "fairseq2.data.DataPipeline.__iter__"]], "concat() (fairseq2.data.datapipeline static method)": [[18, "fairseq2.data.DataPipeline.concat"]], "is_broken (fairseq2.data.datapipeline property)": [[18, "fairseq2.data.DataPipeline.is_broken"]], "load_state_dict() (fairseq2.data.datapipeline method)": [[18, "fairseq2.data.DataPipeline.load_state_dict"]], "reset() (fairseq2.data.datapipeline method)": [[18, "fairseq2.data.DataPipeline.reset"]], "round_robin() (fairseq2.data.datapipeline static method)": [[18, "fairseq2.data.DataPipeline.round_robin"]], "sample() (fairseq2.data.datapipeline static method)": [[18, "fairseq2.data.DataPipeline.sample"]], "state_dict() (fairseq2.data.datapipeline method)": [[18, "fairseq2.data.DataPipeline.state_dict"]], "zip() (fairseq2.data.datapipeline static method)": [[18, "fairseq2.data.DataPipeline.zip"]], "datapipelinebuilder (class in fairseq2.data)": [[19, "fairseq2.data.DataPipelineBuilder"]], "and_return() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.and_return"]], "bucket() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.bucket"]], "bucket_by_length() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.bucket_by_length"]], "collate() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.collate"]], "filter() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.filter"]], "map() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.map"]], "prefetch() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.prefetch"]], "shard() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.shard"]], "shuffle() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.shuffle"]], "skip() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.skip"]], "take() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.take"]], "yield_from() (fairseq2.data.datapipelinebuilder method)": [[19, "fairseq2.data.DataPipelineBuilder.yield_from"]], "datapipelineerror": [[20, "fairseq2.data.DataPipelineError"]], "filemapper (class in fairseq2.data)": [[21, "fairseq2.data.FileMapper"]], "__call__() (fairseq2.data.filemapper method)": [[21, "fairseq2.data.FileMapper.__call__"]], "pathlike (in module fairseq2.data)": [[22, "fairseq2.data.PathLike"]], "recorderror": [[23, "fairseq2.data.RecordError"]], "stringlike (in module fairseq2.data)": [[24, "fairseq2.data.StringLike"]], "vocabularyinfo (class in fairseq2.data)": [[25, "fairseq2.data.VocabularyInfo"]], "bos_idx (fairseq2.data.vocabularyinfo attribute)": [[25, "fairseq2.data.VocabularyInfo.bos_idx"]], "eos_idx (fairseq2.data.vocabularyinfo attribute)": [[25, "fairseq2.data.VocabularyInfo.eos_idx"]], "pad_idx (fairseq2.data.vocabularyinfo attribute)": [[25, "fairseq2.data.VocabularyInfo.pad_idx"]], "size (fairseq2.data.vocabularyinfo attribute)": [[25, "fairseq2.data.VocabularyInfo.size"]], "unk_idx (fairseq2.data.vocabularyinfo attribute)": [[25, "fairseq2.data.VocabularyInfo.unk_idx"]], "get_last_failed_example() (in module fairseq2.data)": [[26, "fairseq2.data.get_last_failed_example"]], "is_string_like() (in module fairseq2.data)": [[27, "fairseq2.data.is_string_like"]], "list_files() (in module fairseq2.data)": [[28, "fairseq2.data.list_files"]], "read_sequence() (in module fairseq2.data)": [[29, "fairseq2.data.read_sequence"]], "read_zipped_records() (in module fairseq2.data)": [[30, "fairseq2.data.read_zipped_records"]], "read_text() (in module fairseq2.data.text)": [[31, "fairseq2.data.text.read_text"]], "lineending (class in fairseq2.data.text)": [[32, "fairseq2.data.text.LineEnding"]], "__iter__() (fairseq2.data.text.lineending class method)": [[32, "fairseq2.data.text.LineEnding.__iter__"]], "multilingualtexttokenizer (class in fairseq2.data.text)": [[33, "fairseq2.data.text.MultilingualTextTokenizer"]], "create_decoder() (fairseq2.data.text.multilingualtexttokenizer method)": [[33, "fairseq2.data.text.MultilingualTextTokenizer.create_decoder"]], "create_encoder() (fairseq2.data.text.multilingualtexttokenizer method)": [[33, "fairseq2.data.text.MultilingualTextTokenizer.create_encoder"]], "sentencepiecedecoder (class in fairseq2.data.text)": [[34, "fairseq2.data.text.SentencePieceDecoder"]], "__call__() (fairseq2.data.text.sentencepiecedecoder method)": [[34, "fairseq2.data.text.SentencePieceDecoder.__call__"]], "sentencepieceencoder (class in fairseq2.data.text)": [[35, "fairseq2.data.text.SentencePieceEncoder"]], "__call__() (fairseq2.data.text.sentencepieceencoder method)": [[35, "fairseq2.data.text.SentencePieceEncoder.__call__"]], "prefix_indices (fairseq2.data.text.sentencepieceencoder property)": [[35, "fairseq2.data.text.SentencePieceEncoder.prefix_indices"]], "suffix_indices (fairseq2.data.text.sentencepieceencoder property)": [[35, "fairseq2.data.text.SentencePieceEncoder.suffix_indices"]], "sentencepiecemodel (class in fairseq2.data.text)": [[36, "fairseq2.data.text.SentencePieceModel"]], "strsplitter (class in fairseq2.data.text)": [[37, "fairseq2.data.text.StrSplitter"]], "__call__() (fairseq2.data.text.strsplitter method)": [[37, "fairseq2.data.text.StrSplitter.__call__"]], "strtointconverter (class in fairseq2.data.text)": [[38, "fairseq2.data.text.StrToIntConverter"]], "__call__() (fairseq2.data.text.strtointconverter method)": [[38, "fairseq2.data.text.StrToIntConverter.__call__"]], "strtotensorconverter (class in fairseq2.data.text)": [[39, "fairseq2.data.text.StrToTensorConverter"]], "__call__() (fairseq2.data.text.strtotensorconverter method)": [[39, "fairseq2.data.text.StrToTensorConverter.__call__"]], "texttokendecoder (class in fairseq2.data.text)": [[40, "fairseq2.data.text.TextTokenDecoder"]], "__call__() (fairseq2.data.text.texttokendecoder method)": [[40, "fairseq2.data.text.TextTokenDecoder.__call__"]], "texttokenencoder (class in fairseq2.data.text)": [[41, "fairseq2.data.text.TextTokenEncoder"]], "__call__() (fairseq2.data.text.texttokenencoder method)": [[41, "fairseq2.data.text.TextTokenEncoder.__call__"]], "prefix_indices (fairseq2.data.text.texttokenencoder property)": [[41, "fairseq2.data.text.TextTokenEncoder.prefix_indices"]], "suffix_indices (fairseq2.data.text.texttokenencoder property)": [[41, "fairseq2.data.text.TextTokenEncoder.suffix_indices"]], "texttokenizer (class in fairseq2.data.text)": [[42, "fairseq2.data.text.TextTokenizer"]], "create_decoder() (fairseq2.data.text.texttokenizer method)": [[42, "fairseq2.data.text.TextTokenizer.create_decoder"]], "create_encoder() (fairseq2.data.text.texttokenizer method)": [[42, "fairseq2.data.text.TextTokenizer.create_encoder"]], "vocabulary_from_sentencepiece() (in module fairseq2.data.text)": [[43, "fairseq2.data.text.vocabulary_from_sentencepiece"]], "post (fairseq2.nn.transformer.transformernormorder attribute)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.POST"]], "pre (fairseq2.nn.transformer.transformernormorder attribute)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.PRE"]], "pre_with_normformer (fairseq2.nn.transformer.transformernormorder attribute)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.PRE_WITH_NORMFORMER"]], "transformernormorder (class in fairseq2.nn.transformer)": [[44, "fairseq2.nn.transformer.TransformerNormOrder"]], "__iter__() (fairseq2.nn.transformer.transformernormorder class method)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.__iter__"]], "to_float_mask() (in module fairseq2.nn.utils.mask)": [[45, "fairseq2.nn.utils.mask.to_float_mask"]]}})