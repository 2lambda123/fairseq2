@article{DBLP:journals/corr/VaswaniSPUJGKP17,
  author     = {Ashish Vaswani and
                Noam Shazeer and
                Niki Parmar and
                Jakob Uszkoreit and
                Llion Jones and
                Aidan N. Gomez and
                Lukasz Kaiser and
                Illia Polosukhin},
  title      = {Attention Is All You Need},
  journal    = {CoRR},
  volume     = {abs/1706.03762},
  year       = {2017},
  url        = {http://arxiv.org/abs/1706.03762},
  eprinttype = {arXiv},
  eprint     = {1706.03762},
  timestamp  = {Sat, 23 Jan 2021 01:20:40 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/VaswaniSPUJGKP17.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2108-12409,
  author     = {Ofir Press and
                Noah A. Smith and
                Mike Lewis},
  title      = {Train Short, Test Long: Attention with Linear Biases Enables Input
                Length Extrapolation},
  journal    = {CoRR},
  volume     = {abs/2108.12409},
  year       = {2021},
  url        = {https://arxiv.org/abs/2108.12409},
  eprinttype = {arXiv},
  eprint     = {2108.12409},
  timestamp  = {Thu, 02 Sep 2021 14:42:29 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2108-12409.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2110-09456,
  author     = {Sam Shleifer and
                Jason Weston and
                Myle Ott},
  title      = {NormFormer: Improved Transformer Pretraining with Extra Normalization},
  journal    = {CoRR},
  volume     = {abs/2110.09456},
  year       = {2021},
  url        = {https://arxiv.org/abs/2110.09456},
  eprinttype = {arXiv},
  eprint     = {2110.09456},
  timestamp  = {Mon, 25 Oct 2021 20:07:12 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2110-09456.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-2002-04745,
  author     = {Ruibin Xiong and
                Yunchang Yang and
                Di He and
                Kai Zheng and
                Shuxin Zheng and
                Chen Xing and
                Huishuai Zhang and
                Yanyan Lan and
                Liwei Wang and
                Tie{-}Yan Liu},
  title      = {On Layer Normalization in the Transformer Architecture},
  journal    = {CoRR},
  volume     = {abs/2002.04745},
  year       = {2020},
  url        = {https://arxiv.org/abs/2002.04745},
  eprinttype = {arXiv},
  eprint     = {2002.04745},
  timestamp  = {Sat, 23 Jan 2021 01:14:26 +0100},
  biburl     = {https://dblp.org/rec/journals/corr/abs-2002-04745.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1909-11556,
  author     = {Angela Fan and
                Edouard Grave and
                Armand Joulin},
  title      = {Reducing Transformer Depth on Demand with Structured Dropout},
  journal    = {CoRR},
  volume     = {abs/1909.11556},
  year       = {2019},
  url        = {http://arxiv.org/abs/1909.11556},
  eprinttype = {arXiv},
  eprint     = {1909.11556},
  timestamp  = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/abs-1909-11556.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
