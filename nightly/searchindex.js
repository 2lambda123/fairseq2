Search.setIndex({"docnames": ["bibliography", "index", "installation/from_source", "reference/abc", "reference/all", "reference/classes", "reference/enums", "reference/functions", "reference/generated/abc/fairseq2.gang.Gang", "reference/generated/abc/fairseq2.nn.PositionEncoder", "reference/generated/abc/fairseq2.nn.Projection", "reference/generated/abc/fairseq2.nn.transformer.AttentionMaskGenerator", "reference/generated/abc/fairseq2.nn.transformer.AttentionWeightHook", "reference/generated/abc/fairseq2.nn.transformer.FeedForwardNetwork", "reference/generated/abc/fairseq2.nn.transformer.MultiheadAttention", "reference/generated/abc/fairseq2.nn.transformer.SDPA", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoder", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoderLayer", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoder", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoderLayer", "reference/generated/classes/fairseq2.nn.Embedding", "reference/generated/classes/fairseq2.nn.IncrementalState", "reference/generated/classes/fairseq2.nn.IncrementalStateBag", "reference/generated/classes/fairseq2.nn.LearnedPositionEncoder", "reference/generated/classes/fairseq2.nn.Linear", "reference/generated/classes/fairseq2.nn.ModuleList", "reference/generated/classes/fairseq2.nn.RotaryEncoder", "reference/generated/classes/fairseq2.nn.SinusoidalPositionEncoder", "reference/generated/classes/fairseq2.nn.TiedProjection", "reference/generated/classes/fairseq2.nn.transformer.ALiBiAttentionMaskGenerator", "reference/generated/classes/fairseq2.nn.transformer.CausalAttentionMaskGenerator", "reference/generated/classes/fairseq2.nn.transformer.MultiheadAttentionState", "reference/generated/classes/fairseq2.nn.transformer.RelativePositionSDPA", "reference/generated/classes/fairseq2.nn.transformer.StandardFeedForwardNetwork", "reference/generated/classes/fairseq2.nn.transformer.StandardMultiheadAttention", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoder", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoderLayer", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoder", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoderLayer", "reference/generated/classes/fairseq2.nn.transformer.StoreAttentionWeights", "reference/generated/classes/fairseq2.optim.lr_scheduler.CosineAnnealingLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.LRSchedulerBase", "reference/generated/classes/fairseq2.optim.lr_scheduler.MyleLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.NoamLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.PolynomialDecayLR", "reference/generated/enums/fairseq2.nn.transformer.TransformerNormOrder", "reference/generated/functions/fairseq2.nn.utils.mask.to_float_mask", "reference/generated/functions/fairseq2.nn.utils.mask.to_padding_mask"], "filenames": ["bibliography.rst", "index.rst", "installation/from_source.rst", "reference/abc.rst", "reference/all.rst", "reference/classes.rst", "reference/enums.rst", "reference/functions.rst", "reference/generated/abc/fairseq2.gang.Gang.rst", "reference/generated/abc/fairseq2.nn.PositionEncoder.rst", "reference/generated/abc/fairseq2.nn.Projection.rst", "reference/generated/abc/fairseq2.nn.transformer.AttentionMaskGenerator.rst", "reference/generated/abc/fairseq2.nn.transformer.AttentionWeightHook.rst", "reference/generated/abc/fairseq2.nn.transformer.FeedForwardNetwork.rst", "reference/generated/abc/fairseq2.nn.transformer.MultiheadAttention.rst", "reference/generated/abc/fairseq2.nn.transformer.SDPA.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoder.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoderLayer.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoder.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoderLayer.rst", "reference/generated/classes/fairseq2.nn.Embedding.rst", "reference/generated/classes/fairseq2.nn.IncrementalState.rst", "reference/generated/classes/fairseq2.nn.IncrementalStateBag.rst", "reference/generated/classes/fairseq2.nn.LearnedPositionEncoder.rst", "reference/generated/classes/fairseq2.nn.Linear.rst", "reference/generated/classes/fairseq2.nn.ModuleList.rst", "reference/generated/classes/fairseq2.nn.RotaryEncoder.rst", "reference/generated/classes/fairseq2.nn.SinusoidalPositionEncoder.rst", "reference/generated/classes/fairseq2.nn.TiedProjection.rst", "reference/generated/classes/fairseq2.nn.transformer.ALiBiAttentionMaskGenerator.rst", "reference/generated/classes/fairseq2.nn.transformer.CausalAttentionMaskGenerator.rst", "reference/generated/classes/fairseq2.nn.transformer.MultiheadAttentionState.rst", "reference/generated/classes/fairseq2.nn.transformer.RelativePositionSDPA.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardFeedForwardNetwork.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardMultiheadAttention.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoder.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoderLayer.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoder.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoderLayer.rst", "reference/generated/classes/fairseq2.nn.transformer.StoreAttentionWeights.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.CosineAnnealingLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.LRSchedulerBase.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.MyleLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.NoamLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.PolynomialDecayLR.rst", "reference/generated/enums/fairseq2.nn.transformer.TransformerNormOrder.rst", "reference/generated/functions/fairseq2.nn.utils.mask.to_float_mask.rst", "reference/generated/functions/fairseq2.nn.utils.mask.to_padding_mask.rst"], "titles": ["Bibliography", "fairseq2 documentation", "Install From Source (C++/CUDA)", "ABCs and Protocols", "All", "Classes", "Enums", "Functions", "Gang", "PositionEncoder", "Projection", "AttentionMaskGenerator", "AttentionWeightHook", "FeedForwardNetwork", "MultiheadAttention", "SDPA", "TransformerDecoder", "TransformerDecoderLayer", "TransformerEncoder", "TransformerEncoderLayer", "Embedding", "IncrementalState", "IncrementalStateBag", "LearnedPositionEncoder", "Linear", "ModuleList", "RotaryEncoder", "SinusoidalPositionEncoder", "TiedProjection", "ALiBiAttentionMaskGenerator", "CausalAttentionMaskGenerator", "MultiheadAttentionState", "RelativePositionSDPA", "StandardFeedForwardNetwork", "StandardMultiheadAttention", "StandardTransformerDecoder", "StandardTransformerDecoderLayer", "StandardTransformerEncoder", "StandardTransformerEncoderLayer", "StoreAttentionWeights", "CosineAnnealingLR", "LRSchedulerBase", "MyleLR", "NoamLR", "PolynomialDecayLR", "TransformerNormOrder", "to_float_mask", "to_padding_mask"], "terms": {"dyi": [0, 32], "19": [0, 32], "zihang": 0, "dai": [0, 32], "zhilin": 0, "yang": 0, "yime": 0, "jaim": 0, "carbonel": 0, "quoc": 0, "v": [0, 14, 15, 31, 32, 34], "le": 0, "ruslan": 0, "salakhutdinov": 0, "transform": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 24, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45], "xl": 0, "attent": [0, 11, 12, 14, 15, 16, 17, 29, 30, 31, 32, 34, 35, 36, 38, 39], "languag": [0, 1], "model": [0, 1, 11, 13, 14, 16, 17, 18, 19, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 43], "beyond": 0, "fix": [0, 20, 27], "length": [0, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 23, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 47], "context": 0, "2019": 0, "arxiv": 0, "1901": 0, "02860": 0, "fgj19": [0, 35, 37], "angela": 0, "fan": [0, 35, 37], "edouard": 0, "grave": 0, "armand": 0, "joulin": 0, "reduc": [0, 8], "depth": 0, "demand": 0, "structur": 0, "dropout": [0, 15, 32, 33, 36, 38], "url": 0, "http": [0, 2], "org": 0, "ab": 0, "1909": 0, "11556": 0, "doi": 0, "10": [0, 30], "48550": 0, "lh17": [0, 40], "ilya": 0, "loshchilov": [0, 40], "frank": 0, "hutter": [0, 40], "sgdr": 0, "stochast": 0, "gradient": [0, 20], "descent": 0, "warm": 0, "restart": [0, 40], "2017": 0, "1608": 0, "03983": 0, "psl21": [0, 29], "ofir": 0, "press": [0, 29], "noah": 0, "A": [0, 14, 21, 31, 34], "smith": 0, "mike": 0, "lewi": 0, "train": [0, 1, 20, 25, 40, 42, 43, 44], "short": [0, 2], "test": [0, 2], "long": [0, 21], "linear": [0, 10, 28], "bias": 0, "enabl": 0, "input": [0, 8, 9, 10, 20, 21, 23, 24, 26, 27, 28], "extrapol": 0, "2021": 0, "2108": 0, "12409": 0, "swo21": [0, 34, 36, 38, 45], "sam": 0, "shleifer": [0, 34, 36, 38, 45], "jason": 0, "weston": 0, "myle": [0, 42], "ott": [0, 42], "normform": 0, "improv": 0, "pretrain": 0, "extra": [0, 25], "normal": [0, 33, 35, 36, 37, 38, 45], "2110": 0, "09456": 0, "slp": [0, 26], "21": [0, 26], "jianlin": 0, "su": [0, 26], "yu": 0, "lu": 0, "shengfeng": 0, "pan": 0, "ahm": 0, "murtadha": 0, "bo": 0, "wen": 0, "yunfeng": 0, "liu": 0, "roform": 0, "enhanc": 0, "rotari": 0, "posit": [0, 9, 14, 22, 23, 26, 27, 32, 34], "embed": [0, 23], "2104": 0, "09864": 0, "vsp": [0, 27, 33, 34, 35, 36, 37, 38, 43, 45], "17": [0, 27, 33, 34, 35, 36, 37, 38, 43, 45], "ashish": 0, "vaswani": [0, 27, 33, 34, 35, 36, 37, 38, 43, 45], "noam": [0, 42, 43], "shazeer": [0, 43], "niki": 0, "parmar": 0, "jakob": 0, "uszkoreit": 0, "llion": 0, "jone": 0, "aidan": 0, "n": [0, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47], "gomez": 0, "lukasz": 0, "kaiser": 0, "illia": 0, "polosukhin": 0, "i": [0, 1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 47], "all": [0, 1, 8, 10, 22, 24, 25, 28, 40, 42, 44], "you": [0, 2], "need": [0, 21], "1706": 0, "03762": 0, "xyh": [0, 45], "20": [0, 45], "ruibin": 0, "xiong": [0, 45], "yunchang": 0, "di": 0, "he": 0, "kai": 0, "zheng": 0, "shuxin": 0, "chen": 0, "xing": 0, "huishuai": 0, "zhang": 0, "yanyan": 0, "lan": 0, "liwei": 0, "wang": 0, "tie": 0, "yan": 0, "On": 0, "layer": [0, 14, 16, 17, 18, 19, 25, 33, 35, 36, 37, 38, 45], "architectur": 0, "2020": 0, "2002": 0, "04745": 0, "sequenc": [1, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 44, 47], "toolkit": [1, 2], "allow": 1, "research": 1, "develop": 1, "custom": 1, "translat": 1, "summar": 1, "other": [1, 2], "content": 1, "gener": [1, 2, 11, 29, 30, 35], "task": 1, "from": [1, 8, 20, 23, 24, 25, 27, 30, 31, 40, 41, 42, 43, 44], "sourc": [1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "c": 1, "cuda": 1, "abc": [1, 8, 9, 10, 13, 14, 15, 16, 17, 18, 19, 21, 41], "protocol": [1, 11, 12, 29, 30, 39], "class": [1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "enum": [1, 45], "function": [1, 9, 39], "bibliographi": 1, "As": 2, "first": [2, 40, 42, 43, 44], "step": [2, 21, 22, 31, 34, 40, 42, 43, 44], "fairseq2": [2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], "git": 2, "your": 2, "machin": 2, "recurs": 2, "submodul": [2, 25], "github": 2, "com": 2, "facebookresearch": 2, "note": 2, "ask": 2, "third": 2, "parti": 2, "along": 2, "If": [2, 9, 15, 16, 18, 20, 22, 24, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44], "have": 2, "alreadi": 2, "without": 2, "befor": [2, 9, 14, 15, 17, 23, 25, 26, 27, 32, 34, 36, 38, 40], "read": 2, "instruct": 2, "can": [2, 9, 14, 23, 26, 27, 34], "run": [2, 14, 34], "follow": [2, 29, 30, 39], "command": 2, "achiev": 2, "same": [2, 9, 10, 13, 16, 17, 18, 19, 23, 24, 26, 27, 28, 33, 35, 36, 37, 38, 47], "effect": [2, 40], "updat": [2, 20, 40, 42, 43, 44], "init": 2, "In": [2, 40, 43], "simplest": 2, "case": [2, 9, 21, 23, 26, 27, 31], "creat": 2, "an": [2, 11, 14, 21, 22, 24, 25, 29, 30, 31, 33, 34, 35, 40, 41, 42, 43, 44, 47], "empti": [2, 30, 34], "shown": 2, "8": 2, "python3": 2, "m": [2, 11, 12, 13, 14, 16, 17, 18, 19, 22, 23, 27, 29, 30, 33, 34, 35, 36, 37, 38, 39], "venv": 2, "myvenv": 2, "activ": [2, 33], "bin": 2, "out": [2, 10, 24, 28], "document": 2, "learn": [2, 23, 24, 33, 34, 40, 41, 42, 43, 44], "mode": [2, 21], "about": [2, 21, 40], "we": [2, 27], "strongli": 2, "recommend": 2, "new": [2, 21, 31], "scratch": 2, "instead": [2, 27], "reus": 2, "exist": 2, "one": 2, "avoid": 2, "conflict": 2, "ha": [2, 9, 12, 21, 31, 40], "some": 2, "librari": 2, "typic": [2, 21, 31], "via": 2, "nativ": 2, "manag": 2, "For": 2, "ubuntu": 2, "base": [2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45], "sudo": 2, "apt": 2, "libsndfil": 2, "dev": 2, "similarli": 2, "fedora": 2, "dnf": 2, "devel": 2, "plan": 2, "version": [2, 42], "match": [2, 22], "pytorch": 2, "The": [2, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47], "differ": [2, 24, 27], "found": 2, "nvidia": 2, "": [2, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 23, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 38, 45, 47], "websit": 2, "ar": [2, 8, 10, 24, 27, 28], "comput": [2, 12, 14, 15, 17, 31, 32, 34, 36, 40, 41, 42, 43, 44], "cluster": 2, "support": [2, 40], "e": [2, 9, 20, 22, 23, 26, 27, 31, 34, 40], "g": [2, 22, 30, 31], "fair": 2, "specif": [2, 11, 47], "load": [2, 40, 41, 42, 43, 44], "To": 2, "us": [2, 9, 14, 16, 17, 21, 22, 23, 24, 26, 27, 28, 31, 33, 34, 35, 36, 37, 38, 43, 44], "torch": [2, 21, 23, 24, 25, 27, 30, 31, 46], "r": 2, "fairseq2n": 2, "requir": 2, "txt": 2, "final": [2, 20, 23, 25, 26, 27, 28, 32, 33, 34, 35, 36, 37, 38, 40, 42, 43, 44], "its": [2, 30], "root": [2, 42, 43], "directori": 2, "configur": 2, "cd": 2, "cmake": 2, "gninja": 2, "b": 2, "onc": 2, "complet": 2, "reason": 2, "default": [2, 34], "so": 2, "abov": [2, 30], "suffici": 2, "standard": 2, "howev": 2, "familiar": 2, "advanc": 2, "cmakelist": 2, "project": [2, 13, 24, 28, 31, 33, 34], "would": 2, "like": 2, "kernel": 2, "fairseq2n_use_cuda": 2, "ON": 2, "when": [2, 21, 31], "turn": 2, "wa": [2, 42], "must": [2, 21], "dfairseq2n_use_cuda": 2, "similar": 2, "thi": [2, 8, 21, 22, 24, 25, 27, 29, 30, 31, 39, 40, 42, 43, 44], "By": 2, "volta": 2, "overrid": 2, "cmake_cuda_architectur": 2, "instanc": [2, 28, 35], "binari": 2, "ptx": 2, "code": 2, "amper": 2, "a100": 2, "dcmake_cuda_architectur": 2, "80": 2, "real": 2, "built": 2, "actual": 2, "straightforward": 2, "And": 2, "fairseq2n_devel": 2, "make": 2, "sure": 2, "issu": 2, "pytest": 2, "pass": [2, 12], "devic": [2, 8, 20, 23, 24, 26, 27, 32, 33, 34, 35, 36, 37, 38], "form": 2, "d": 2, "argument": 2, "them": [2, 8, 36, 38], "gpu": 2, "0": [2, 15, 20, 23, 25, 27, 30, 32, 33, 35, 36, 37, 38, 40, 42, 44, 45], "rank": 8, "size": [8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47], "repres": [8, 12, 13, 14, 16, 17, 18, 19, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 47], "set": [8, 22, 25], "process": [8, 17, 19, 36, 38], "work": 8, "collect": 8, "paramet": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 42, 43, 44, 46, 47], "int": [8, 9, 10, 13, 14, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 31, 32, 33, 34, 40, 42, 43, 44], "number": [8, 14, 25, 29, 32, 34, 40, 42, 43, 44, 47], "part": [8, 30], "associ": [8, 40, 42, 43, 44], "abstract": [8, 9, 10, 13, 14, 15, 16, 17, 18, 19, 21, 41], "all_gath": 8, "output_tensor": 8, "input_tensor": 8, "gather": 8, "tensor": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 46, 47], "put": 8, "singl": 8, "output": [8, 10, 16, 17, 18, 19, 21, 24, 28, 33, 34, 35, 36, 37, 38], "accomod": 8, "element": [8, 47], "current": [8, 22, 31, 40, 41, 42, 43, 44], "all_reduc": 8, "op": 8, "across": 8, "oper": 8, "reduceoper": 8, "wise": 8, "as_process_group": 8, "return": [8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 46, 47], "group": [8, 40, 41, 42, 43, 44], "type": [8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 45, 46, 47], "processgroup": 8, "barrier": 8, "synchron": 8, "nn": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 45, 46, 47], "encoding_dim": [9, 23, 26, 27], "max_seq_len": [9, 23, 26, 27], "modul": [9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 45], "encod": [9, 16, 17, 18, 19, 23, 26, 27, 32, 34, 35, 36, 37, 38], "inform": [9, 22, 23, 26, 27, 36, 38], "dimension": [9, 10, 11, 13, 14, 16, 17, 18, 19, 20, 23, 24, 26, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 43], "none": [9, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24, 25, 26, 27, 28, 31, 32, 33, 34, 35, 36, 37, 38, 40, 41, 42, 43, 44, 45, 47], "expect": [9, 22, 23, 26, 27], "maximum": [9, 23, 26, 27], "_do_forward": 9, "seq": [9, 11, 13, 16, 17, 18, 19, 23, 26, 27, 29, 30, 33, 35, 36, 37, 38, 47], "padding_mask": [9, 14, 16, 17, 18, 19, 23, 26, 27, 34, 35, 36, 37, 38], "state_bag": [9, 14, 16, 17, 23, 26, 27, 34, 35, 36], "shape": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 46, 47], "where": [9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 23, 24, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 40, 47], "batch": [9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 23, 26, 27, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 47], "float": [9, 14, 15, 16, 17, 18, 19, 23, 25, 26, 27, 31, 32, 33, 34, 35, 36, 37, 38, 40, 42, 44, 46, 47], "pad": [9, 14, 16, 17, 18, 19, 23, 26, 27, 31, 34, 35, 36, 37, 38, 47], "mask": [9, 11, 14, 15, 16, 17, 18, 19, 23, 26, 27, 29, 30, 31, 32, 34, 35, 36, 37, 38, 46, 47], "n_": [9, 23, 26, 27], "msk": [9, 23, 26, 27], "appli": [9, 10, 23, 24, 26, 27, 28, 33, 34, 35, 37, 45], "deriv": 9, "should": [9, 22, 31, 40, 41, 42, 43, 44], "apply_padding_mask": 9, "handl": [9, 14, 34], "tile": [9, 23, 26, 27], "incrementalstatebag": [9, 14, 16, 17, 23, 26, 27, 34, 35, 36], "state": [9, 14, 16, 17, 21, 22, 23, 26, 27, 31, 34, 35, 36, 40, 41, 42, 43, 44], "bag": [9, 14, 16, 17, 22, 23, 26, 27, 34, 35, 36], "increment": [9, 14, 16, 17, 21, 22, 23, 26, 27, 31, 34, 35, 36], "evalu": [9, 14, 16, 17, 21, 22, 23, 26, 27, 31, 34, 35, 36], "forward": [9, 10, 13, 14, 15, 16, 17, 18, 19, 20, 23, 24, 26, 27, 28, 32, 33, 34, 35, 36, 37, 38], "multipl": [9, 23, 26, 27], "which": [9, 11, 14, 22, 23, 26, 27, 29, 30, 34, 39, 40, 41, 42, 43, 44], "being": [9, 23, 26, 27, 40], "input_dim": [10, 24], "output_dim": [10, 24], "incom": [10, 24, 28], "data": [10, 24, 28], "x": [10, 20, 24, 25, 28], "h_": [10, 24, 28], "inp": [10, 24, 28], "last": [10, 24, 28, 40, 41, 42, 43, 44], "dimens": [10, 24, 28, 47], "arg": [11, 12, 25, 40, 41, 42, 43, 44], "kwarg": [11, 12], "__call__": [11, 12, 29, 30, 39], "implement": [11, 34, 40, 42], "defin": 11, "hook": [12, 14, 34], "register_attn_weight_hook": [12, 14, 34], "attn_weight": [12, 14, 34, 39], "multiheadattent": [12, 31, 34, 36, 38], "weight": [12, 14, 15, 17, 24, 28, 32, 34, 36, 39], "s_": [12, 14, 15, 16, 17, 31, 32, 34, 35, 36], "kv": [12, 14, 15, 32, 34], "kei": [12, 14, 15, 31, 32, 34], "valu": [12, 14, 15, 22, 31, 32, 34, 40, 45], "model_dim": [13, 14, 16, 17, 18, 19, 32, 33, 34], "feed": [13, 33, 36, 38], "network": [13, 33, 36, 38], "num_head": [14, 29, 32, 34], "multi": [14, 34], "head": [14, 29, 32, 34], "_run_attn_weight_hook": [14, 34], "regist": [14, 34], "queri": [14, 15, 32, 34], "attn_mask": [14, 34], "key_padding_mask": [14, 31, 34], "k": [14, 15, 24, 31, 32, 34], "ad": [14, 15, 17, 32, 34, 36, 38], "indic": [14, 20, 34], "ignor": [14, 34], "purpos": [14, 34], "call": [14, 16, 18, 21, 22, 31, 34, 35, 37, 39, 40, 41, 42, 43, 44], "everi": [14, 22, 25, 34, 40, 41, 42, 43, 44], "time": [14, 34], "after": [14, 22, 34, 40, 43, 44, 45], "attentionweighthook": [14, 34, 39], "remov": [14, 34], "removablehandl": [14, 34], "attn_dropout_p": [15, 32], "scale": [15, 20, 32, 34, 36, 38, 40, 42], "dot": [15, 32, 34], "product": [15, 32, 34], "probabl": [15, 25, 32, 33, 36, 38], "needs_weight": [15, 32], "fals": [15, 20, 24, 32, 34, 36, 38, 40, 41, 42, 43, 44], "bool": [15, 20, 24, 32, 33, 34, 36, 38, 40, 42, 43, 44], "true": [15, 20, 24, 32, 33, 34, 36, 38, 40, 42, 43, 44], "tupl": [15, 16, 17, 18, 19, 31, 32, 35, 36, 37, 38], "decod": [16, 17, 35, 36], "encoder_output": [16, 17, 35, 36], "encoder_padding_mask": [16, 17, 35, 36], "layer_output_hook": [16, 18, 35, 37], "enc": [16, 17, 35, 36], "m_": [16, 17, 35, 36], "encoder_out": [16, 35], "decoderlayeroutputhook": [16, 35], "each": [16, 18, 35, 37, 40, 42, 43, 44, 45, 47], "stack": [16, 18, 35, 37], "self_attn_mask": [17, 36], "self": [17, 29, 30, 36, 38, 39, 40, 41, 42, 43, 44], "encoderlayeroutputhook": [18, 37], "num_embed": 20, "embedding_dim": 20, "pad_idx": 20, "dtype": [20, 23, 24, 26, 27, 32, 33, 34, 35, 36, 37, 38, 46], "store": [20, 22, 39], "dictionari": 20, "tabl": [20, 32], "entri": [20, 40, 41, 42, 43, 44], "do": 20, "contribut": 20, "therefor": 20, "dure": [20, 21, 22, 25, 31, 40, 44], "initi": [20, 24, 27, 31, 40, 42, 44], "mathcal": [20, 24], "frac": [20, 24, 27, 40, 42, 43, 44], "1": [20, 22, 23, 24, 25, 27, 31, 36, 38, 40, 41, 42, 43, 44, 45], "text": [20, 24, 27, 40], "otherwis": 20, "ani": [20, 21, 25, 46, 47], "correspond": [20, 21, 42, 43, 44], "specifi": [20, 34, 45], "reset_paramet": [20, 23, 24, 26, 27, 32, 34, 36, 38], "reset": [20, 23, 24, 26, 27, 32, 34, 36, 38], "buffer": [20, 23, 24, 26, 27, 32, 34, 36, 38], "hold": [21, 22, 25, 31], "special": 21, "onli": 21, "receiv": 21, "previou": 21, "produc": [21, 34], "next": 21, "thu": 21, "cach": 21, "term": 21, "reorder": [21, 22, 31], "new_ord": [21, 22, 31], "rearrang": [21, 31], "accord": [21, 31], "order": [21, 31, 33, 35, 36, 37, 38, 45], "chang": [21, 31], "beam": [21, 22, 31], "search": [21, 22, 31], "between": [21, 31], "select": [21, 31], "It": [21, 22, 31, 40, 41, 42, 43, 44], "frequent": [21, 31], "index_select": [21, 31], "object": [22, 29, 30, 39, 40, 41, 42, 43, 44], "get_stat": 22, "kl": 22, "get": 22, "present": 22, "t": [22, 40, 42, 43, 44], "doe": 22, "increment_step": 22, "delta": 22, "method": [22, 24], "keep": 22, "track": 22, "see": [22, 27, 36, 38], "incrementalst": [22, 31], "more": [22, 27, 36, 38, 40], "set_stat": 22, "properti": 22, "positionencod": [23, 26, 27, 34], "usag": [23, 25, 27, 30], "import": [23, 25, 27, 30], "position_encod": [23, 27], "4": [23, 27, 30], "16": [23, 27], "ones": [23, 27], "3": [23, 25, 27, 30, 43], "1135": 23, "5548": 23, "4293": 23, "2": [23, 27, 40, 45], "0112": 23, "po": [23, 27], "2364": 23, "6009": 23, "3865": 23, "4810": 23, "4746": 23, "4544": 23, "2761": 23, "8828": 23, "grad_fn": 23, "squeezebackward1": 23, "bia": [24, 28, 33, 34], "skip_init": 24, "unless": 24, "overridden": 24, "subclass": 24, "u": 24, "sqrt": [24, 42, 43], "ident": 24, "addit": [24, 33, 34, 40], "left": 24, "uniniti": 24, "becom": 24, "noop": 24, "intend": 24, "author": [24, 43], "who": 24, "want": 24, "drop_p": 25, "list": 25, "extend": [25, 34], "featur": 25, "option": 25, "drop": 25, "random": 25, "iter": 25, "layer1": 25, "layer2": 25, "layer3": 25, "5": [25, 27, 43], "drop_it": 25, "might": 25, "over": [25, 44], "add": 25, "append": [25, 31], "given": 25, "end": [25, 40], "python": 25, "insert": 25, "index": [25, 40, 42, 43, 44, 47], "rel": [26, 32, 40], "describ": [26, 29, 32, 33, 34, 35, 36, 37, 38, 40, 43, 45], "et": [26, 27, 29, 32, 33, 34, 35, 36, 37, 38, 43, 45], "al": [26, 27, 29, 32, 33, 34, 35, 36, 37, 38, 43, 45], "reset_non_persistent_buff": [26, 27], "non": [26, 27], "persist": [26, 27], "_legacy_pad_idx": 27, "sinusoid": 27, "tensor2tensor": 27, "slightli": 27, "descript": 27, "section": [27, 43], "mean": 27, "pe_": 27, "2i": 27, "sin": 27, "10000": 27, "d_": 27, "co": [27, 40], "geq": 27, "here": 27, "0000e": 27, "00": 27, "9": 27, "4147e": 27, "01": 27, "04": 27, "6": 27, "4030e": 27, "0930e": 27, "02": 27, "1615e": 27, "anoth": 28, "share": 28, "attentionmaskgener": [29, 30, 35], "alibi": 29, "h": 29, "causal": 30, "whose": 30, "upper": 30, "triangular": 30, "main": 30, "diagon": 30, "fill": 30, "neg": 30, "infin": 30, "while": 30, "rest": 30, "zero": [30, 34, 35, 37], "inf": 30, "k_": 31, "proj": 31, "v_": 31, "prev_k": 31, "prev_v": 31, "stp": 31, "accumul": 31, "past": 31, "prv": 31, "prev_key_padding_mask": 31, "pos_encod": [32, 34], "sdpa": [32, 34], "param": 32, "inner_dim": 33, "inner_activ": 33, "inner_dropout_p": 33, "norm_ord": [33, 35, 36, 37, 38], "transformernormord": [33, 35, 36, 37, 38], "post": [33, 35, 36, 37, 38, 45], "layer_norm_fn": [33, 35, 36, 37, 38], "feedforwardnetwork": [33, 36, 38], "inner": 33, "relu": 33, "both": 33, "layernormfactori": [33, 35, 36, 37, 38], "factori": [33, 35, 36, 37, 38], "construct": [33, 35, 36, 37, 38], "q_proj": 34, "k_proj": 34, "v_proj": 34, "add_bias_kv": 34, "add_zero_attn": 34, "scale_head": 34, "output_proj": 34, "explicitli": 34, "self_attn_mask_gen": 35, "layer_drop_p": [35, 37], "transformerdecod": 35, "modulelist": [35, 37], "causalattentionmaskgener": 35, "greater": [35, 37], "than": [35, 37], "layerdrop": [35, 37], "self_attn": [36, 38], "encoder_decoder_attn": 36, "ffn": [36, 38], "scale_residu": [36, 38], "dropout_p": [36, 38], "transformerdecoderlay": 36, "residu": [36, 38, 45], "transformerencod": 37, "transformerencoderlay": 38, "storag": 39, "provid": 39, "mutablesequ": 39, "optim": [40, 41, 42, 43, 44], "lr_schedul": [40, 41, 42, 43, 44], "cycle_len": 40, "num_warmup_step": [40, 42, 43, 44], "cycle_mul": 40, "lr_mul": 40, "start_lr": [40, 42, 44], "final_lr": [40, 44], "last_epoch": [40, 41, 42, 43, 44], "verbos": [40, 41, 42, 43, 44], "lrschedulerbas": [40, 42, 43, 44], "rate": [40, 41, 42, 43, 44], "schedul": [40, 41, 42, 43, 44], "warmup": [40, 42, 43, 44], "eta_t": [40, 42, 43, 44], "eta_": [40, 42, 43, 44], "t_": [40, 42, 43, 44], "pi": 40, "anneal": 40, "cycl": 40, "t_i": 40, "taken": 40, "sinc": 40, "total": [40, 44], "within": 40, "th": 40, "cosin": 40, "start": [40, 45], "larg": 40, "rapidli": 40, "decreas": [40, 42, 43, 44], "minimum": 40, "increas": [40, 42, 43, 44], "again": 40, "pleas": 40, "refer": [40, 43], "paper": [40, 43], "detail": 40, "origin": [40, 42], "also": 40, "phase": 40, "linearli": [40, 42, 43, 44], "chainabl": [40, 42, 43, 44], "factor": 40, "grow": 40, "respect": [40, 42, 44], "epoch": [40, 41, 42, 43, 44], "print": [40, 42, 43, 44], "messag": [40, 42, 43, 44], "stdout": [40, 42, 43, 44], "get_last_lr": [40, 41, 42, 43, 44], "load_state_dict": [40, 41, 42, 43, 44], "state_dict": [40, 41, 42, 43, 44], "dict": [40, 41, 42, 43, 44], "print_lr": [40, 41, 42, 43, 44], "is_verbos": [40, 41, 42, 43, 44], "lr": [40, 41, 42, 43, 44], "displai": [40, 41, 42, 43, 44], "contain": [40, 41, 42, 43, 44], "variabl": [40, 41, 42, 43, 44], "__dict__": [40, 41, 42, 43, 44], "_lrschedul": 41, "noamlr": 42, "preserv": 42, "min": [42, 43], "essenti": 42, "squar": [42, 43], "propos": 42, "fairseq": 42, "under": 42, "name": [42, 45], "inversesquarerootlr": 42, "thereaft": [42, 43, 44], "proportion": [42, 43], "invers": [42, 43], "commonli": 43, "second": 43, "num_step": 44, "power": 44, "polynomi": 44, "decai": 44, "p": 44, "degre": 44, "includ": [44, 47], "expon": 44, "qualnam": 45, "boundari": 45, "classmethod": 45, "__iter__": 45, "member": 45, "definit": 45, "connect": 45, "pre": 45, "begin": 45, "pre_with_normform": 45, "util": [46, 47], "float32": 46, "convert": [46, 47], "boolean": 46, "point": 46, "seq_len": 47, "arrai": 47}, "objects": {"fairseq2.gang": [[8, 0, 1, "", "Gang"]], "fairseq2.gang.Gang": [[8, 1, 1, "", "all_gather"], [8, 1, 1, "", "all_reduce"], [8, 1, 1, "", "as_process_group"], [8, 1, 1, "", "barrier"]], "fairseq2.nn": [[20, 0, 1, "", "Embedding"], [21, 0, 1, "", "IncrementalState"], [22, 0, 1, "", "IncrementalStateBag"], [23, 0, 1, "", "LearnedPositionEncoder"], [24, 0, 1, "", "Linear"], [25, 0, 1, "", "ModuleList"], [9, 0, 1, "", "PositionEncoder"], [10, 0, 1, "", "Projection"], [26, 0, 1, "", "RotaryEncoder"], [27, 0, 1, "", "SinusoidalPositionEncoder"], [28, 0, 1, "", "TiedProjection"]], "fairseq2.nn.Embedding": [[20, 1, 1, "", "forward"], [20, 1, 1, "", "reset_parameters"]], "fairseq2.nn.IncrementalState": [[21, 1, 1, "", "reorder"]], "fairseq2.nn.IncrementalStateBag": [[22, 1, 1, "", "get_state"], [22, 1, 1, "", "increment_step"], [22, 1, 1, "", "reorder"], [22, 1, 1, "", "set_state"], [22, 2, 1, "", "step"]], "fairseq2.nn.LearnedPositionEncoder": [[23, 1, 1, "", "forward"], [23, 1, 1, "", "reset_parameters"]], "fairseq2.nn.Linear": [[24, 1, 1, "", "forward"], [24, 1, 1, "", "reset_parameters"]], "fairseq2.nn.ModuleList": [[25, 1, 1, "", "append"], [25, 1, 1, "", "drop_iter"], [25, 1, 1, "", "extend"], [25, 1, 1, "", "insert"]], "fairseq2.nn.PositionEncoder": [[9, 1, 1, "", "_do_forward"], [9, 1, 1, "", "forward"]], "fairseq2.nn.Projection": [[10, 1, 1, "", "forward"]], "fairseq2.nn.RotaryEncoder": [[26, 1, 1, "", "forward"], [26, 1, 1, "", "reset_non_persistent_buffers"], [26, 1, 1, "", "reset_parameters"]], "fairseq2.nn.SinusoidalPositionEncoder": [[27, 1, 1, "", "forward"], [27, 1, 1, "", "reset_non_persistent_buffers"], [27, 1, 1, "", "reset_parameters"]], "fairseq2.nn.TiedProjection": [[28, 1, 1, "", "forward"]], "fairseq2.nn.transformer": [[29, 0, 1, "", "ALiBiAttentionMaskGenerator"], [11, 0, 1, "", "AttentionMaskGenerator"], [12, 0, 1, "", "AttentionWeightHook"], [30, 0, 1, "", "CausalAttentionMaskGenerator"], [13, 0, 1, "", "FeedForwardNetwork"], [14, 0, 1, "", "MultiheadAttention"], [31, 0, 1, "", "MultiheadAttentionState"], [32, 0, 1, "", "RelativePositionSDPA"], [15, 0, 1, "", "SDPA"], [33, 0, 1, "", "StandardFeedForwardNetwork"], [34, 0, 1, "", "StandardMultiheadAttention"], [35, 0, 1, "", "StandardTransformerDecoder"], [36, 0, 1, "", "StandardTransformerDecoderLayer"], [37, 0, 1, "", "StandardTransformerEncoder"], [38, 0, 1, "", "StandardTransformerEncoderLayer"], [39, 0, 1, "", "StoreAttentionWeights"], [16, 0, 1, "", "TransformerDecoder"], [17, 0, 1, "", "TransformerDecoderLayer"], [18, 0, 1, "", "TransformerEncoder"], [19, 0, 1, "", "TransformerEncoderLayer"], [45, 0, 1, "", "TransformerNormOrder"]], "fairseq2.nn.transformer.ALiBiAttentionMaskGenerator": [[29, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.AttentionMaskGenerator": [[11, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.AttentionWeightHook": [[12, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.CausalAttentionMaskGenerator": [[30, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.FeedForwardNetwork": [[13, 1, 1, "", "forward"]], "fairseq2.nn.transformer.MultiheadAttention": [[14, 1, 1, "", "_run_attn_weight_hooks"], [14, 1, 1, "", "forward"], [14, 1, 1, "", "register_attn_weight_hook"]], "fairseq2.nn.transformer.MultiheadAttentionState": [[31, 1, 1, "", "append"], [31, 3, 1, "", "prev_k"], [31, 3, 1, "", "prev_key_padding_mask"], [31, 3, 1, "", "prev_v"], [31, 1, 1, "", "reorder"]], "fairseq2.nn.transformer.RelativePositionSDPA": [[32, 1, 1, "", "forward"], [32, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.SDPA": [[15, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardFeedForwardNetwork": [[33, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardMultiheadAttention": [[34, 1, 1, "", "_run_attn_weight_hooks"], [34, 1, 1, "", "forward"], [34, 1, 1, "", "register_attn_weight_hook"], [34, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.StandardTransformerDecoder": [[35, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardTransformerDecoderLayer": [[36, 1, 1, "", "forward"], [36, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.StandardTransformerEncoder": [[37, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardTransformerEncoderLayer": [[38, 1, 1, "", "forward"], [38, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.StoreAttentionWeights": [[39, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.TransformerDecoder": [[16, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerDecoderLayer": [[17, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerEncoder": [[18, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerEncoderLayer": [[19, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerNormOrder": [[45, 3, 1, "", "POST"], [45, 3, 1, "", "PRE"], [45, 3, 1, "", "PRE_WITH_NORMFORMER"], [45, 1, 1, "", "__iter__"]], "fairseq2.nn.utils.mask": [[46, 4, 1, "", "to_float_mask"], [47, 4, 1, "", "to_padding_mask"]], "fairseq2.optim.lr_scheduler": [[40, 0, 1, "", "CosineAnnealingLR"], [41, 0, 1, "", "LRSchedulerBase"], [42, 0, 1, "", "MyleLR"], [43, 0, 1, "", "NoamLR"], [44, 0, 1, "", "PolynomialDecayLR"]], "fairseq2.optim.lr_scheduler.CosineAnnealingLR": [[40, 1, 1, "", "get_last_lr"], [40, 1, 1, "", "load_state_dict"], [40, 1, 1, "", "print_lr"], [40, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.LRSchedulerBase": [[41, 1, 1, "", "get_last_lr"], [41, 1, 1, "", "load_state_dict"], [41, 1, 1, "", "print_lr"], [41, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.MyleLR": [[42, 1, 1, "", "get_last_lr"], [42, 1, 1, "", "load_state_dict"], [42, 1, 1, "", "print_lr"], [42, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.NoamLR": [[43, 1, 1, "", "get_last_lr"], [43, 1, 1, "", "load_state_dict"], [43, 1, 1, "", "print_lr"], [43, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.PolynomialDecayLR": [[44, 1, 1, "", "get_last_lr"], [44, 1, 1, "", "load_state_dict"], [44, 1, 1, "", "print_lr"], [44, 1, 1, "", "state_dict"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:property", "3": "py:attribute", "4": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "property", "Python property"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "function", "Python function"]}, "titleterms": {"bibliographi": 0, "fairseq2": 1, "document": 1, "instal": [1, 2], "nn": 1, "refer": 1, "misc": 1, "from": 2, "sourc": 2, "c": 2, "cuda": 2, "1": 2, "clone": 2, "repositori": 2, "2": 2, "set": 2, "up": 2, "python": 2, "virtual": 2, "environ": 2, "3": 2, "depend": 2, "system": 2, "pip": 2, "4": 2, "build": 2, "extens": 2, "modul": 2, "cpu": 2, "onli": 2, "architectur": 2, "5": 2, "packag": 2, "6": 2, "option": 2, "saniti": 2, "check": 2, "abc": [3, 4], "protocol": [3, 4], "all": 4, "class": [4, 5], "enum": [4, 6], "function": [4, 7], "gang": 8, "positionencod": 9, "project": 10, "attentionmaskgener": 11, "attentionweighthook": 12, "feedforwardnetwork": 13, "multiheadattent": 14, "sdpa": 15, "transformerdecod": 16, "transformerdecoderlay": 17, "transformerencod": 18, "transformerencoderlay": 19, "embed": 20, "incrementalst": 21, "incrementalstatebag": 22, "learnedpositionencod": 23, "linear": 24, "modulelist": 25, "rotaryencod": 26, "sinusoidalpositionencod": 27, "tiedproject": 28, "alibiattentionmaskgener": 29, "causalattentionmaskgener": 30, "multiheadattentionst": 31, "relativepositionsdpa": 32, "standardfeedforwardnetwork": 33, "standardmultiheadattent": 34, "standardtransformerdecod": 35, "standardtransformerdecoderlay": 36, "standardtransformerencod": 37, "standardtransformerencoderlay": 38, "storeattentionweight": 39, "cosineannealinglr": 40, "lrschedulerbas": 41, "mylelr": 42, "noamlr": 43, "polynomialdecaylr": 44, "transformernormord": 45, "to_float_mask": 46, "to_padding_mask": 47}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinxcontrib.bibtex": 9, "sphinx": 57}, "alltitles": {"Bibliography": [[0, "bibliography"]], "fairseq2 documentation": [[1, "fairseq2-documentation"]], "Installation": [[1, null]], "fairseq2.nn Reference": [[1, null]], "Misc": [[1, null]], "Install From Source (C++/CUDA)": [[2, "install-from-source-c-cuda"]], "1. Clone the Repository": [[2, "clone-the-repository"]], "2. Set Up a Python Virtual Environment": [[2, "set-up-a-python-virtual-environment"]], "3. Install the Dependencies": [[2, "install-the-dependencies"]], "System": [[2, "system"]], "CUDA": [[2, "cuda"]], "pip": [[2, "pip"]], "4. Build the Extension Module": [[2, "build-the-extension-module"]], "CPU-Only Builds": [[2, "cpu-only-builds"]], "CUDA Builds": [[2, "cuda-builds"]], "CUDA Architectures": [[2, "cuda-architectures"]], "5. Install the Package": [[2, "install-the-package"]], "6. Optional: Sanity Check": [[2, "optional-sanity-check"]], "ABCs and Protocols": [[3, "abcs-and-protocols"], [4, "abcs-and-protocols"]], "All": [[4, "all"]], "Classes": [[4, "classes"], [5, "classes"]], "Enums": [[4, "enums"], [6, "enums"]], "Functions": [[4, "functions"], [7, "functions"]], "Gang": [[8, "gang"]], "PositionEncoder": [[9, "positionencoder"]], "Projection": [[10, "projection"]], "AttentionMaskGenerator": [[11, "attentionmaskgenerator"]], "AttentionWeightHook": [[12, "attentionweighthook"]], "FeedForwardNetwork": [[13, "feedforwardnetwork"]], "MultiheadAttention": [[14, "multiheadattention"]], "SDPA": [[15, "sdpa"]], "TransformerDecoder": [[16, "transformerdecoder"]], "TransformerDecoderLayer": [[17, "transformerdecoderlayer"]], "TransformerEncoder": [[18, "transformerencoder"]], "TransformerEncoderLayer": [[19, "transformerencoderlayer"]], "Embedding": [[20, "embedding"]], "IncrementalState": [[21, "incrementalstate"]], "IncrementalStateBag": [[22, "incrementalstatebag"]], "LearnedPositionEncoder": [[23, "learnedpositionencoder"]], "Linear": [[24, "linear"]], "ModuleList": [[25, "modulelist"]], "RotaryEncoder": [[26, "rotaryencoder"]], "SinusoidalPositionEncoder": [[27, "sinusoidalpositionencoder"]], "TiedProjection": [[28, "tiedprojection"]], "ALiBiAttentionMaskGenerator": [[29, "alibiattentionmaskgenerator"]], "CausalAttentionMaskGenerator": [[30, "causalattentionmaskgenerator"]], "MultiheadAttentionState": [[31, "multiheadattentionstate"]], "RelativePositionSDPA": [[32, "relativepositionsdpa"]], "StandardFeedForwardNetwork": [[33, "standardfeedforwardnetwork"]], "StandardMultiheadAttention": [[34, "standardmultiheadattention"]], "StandardTransformerDecoder": [[35, "standardtransformerdecoder"]], "StandardTransformerDecoderLayer": [[36, "standardtransformerdecoderlayer"]], "StandardTransformerEncoder": [[37, "standardtransformerencoder"]], "StandardTransformerEncoderLayer": [[38, "standardtransformerencoderlayer"]], "StoreAttentionWeights": [[39, "storeattentionweights"]], "CosineAnnealingLR": [[40, "cosineannealinglr"]], "LRSchedulerBase": [[41, "lrschedulerbase"]], "MyleLR": [[42, "mylelr"]], "NoamLR": [[43, "noamlr"]], "PolynomialDecayLR": [[44, "polynomialdecaylr"]], "TransformerNormOrder": [[45, "transformernormorder"]], "to_float_mask": [[46, "to-float-mask"]], "to_padding_mask": [[47, "to-padding-mask"]]}, "indexentries": {"gang (class in fairseq2.gang)": [[8, "fairseq2.gang.Gang"]], "all_gather() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.all_gather"]], "all_reduce() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.all_reduce"]], "as_process_group() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.as_process_group"]], "barrier() (fairseq2.gang.gang method)": [[8, "fairseq2.gang.Gang.barrier"]], "positionencoder (class in fairseq2.nn)": [[9, "fairseq2.nn.PositionEncoder"]], "_do_forward() (fairseq2.nn.positionencoder method)": [[9, "fairseq2.nn.PositionEncoder._do_forward"]], "forward() (fairseq2.nn.positionencoder method)": [[9, "fairseq2.nn.PositionEncoder.forward"]], "projection (class in fairseq2.nn)": [[10, "fairseq2.nn.Projection"]], "forward() (fairseq2.nn.projection method)": [[10, "fairseq2.nn.Projection.forward"]], "attentionmaskgenerator (class in fairseq2.nn.transformer)": [[11, "fairseq2.nn.transformer.AttentionMaskGenerator"]], "__call__() (fairseq2.nn.transformer.attentionmaskgenerator method)": [[11, "fairseq2.nn.transformer.AttentionMaskGenerator.__call__"]], "attentionweighthook (class in fairseq2.nn.transformer)": [[12, "fairseq2.nn.transformer.AttentionWeightHook"]], "__call__() (fairseq2.nn.transformer.attentionweighthook method)": [[12, "fairseq2.nn.transformer.AttentionWeightHook.__call__"]], "feedforwardnetwork (class in fairseq2.nn.transformer)": [[13, "fairseq2.nn.transformer.FeedForwardNetwork"]], "forward() (fairseq2.nn.transformer.feedforwardnetwork method)": [[13, "fairseq2.nn.transformer.FeedForwardNetwork.forward"]], "multiheadattention (class in fairseq2.nn.transformer)": [[14, "fairseq2.nn.transformer.MultiheadAttention"]], "_run_attn_weight_hooks() (fairseq2.nn.transformer.multiheadattention method)": [[14, "fairseq2.nn.transformer.MultiheadAttention._run_attn_weight_hooks"]], "forward() (fairseq2.nn.transformer.multiheadattention method)": [[14, "fairseq2.nn.transformer.MultiheadAttention.forward"]], "register_attn_weight_hook() (fairseq2.nn.transformer.multiheadattention method)": [[14, "fairseq2.nn.transformer.MultiheadAttention.register_attn_weight_hook"]], "sdpa (class in fairseq2.nn.transformer)": [[15, "fairseq2.nn.transformer.SDPA"]], "forward() (fairseq2.nn.transformer.sdpa method)": [[15, "fairseq2.nn.transformer.SDPA.forward"]], "transformerdecoder (class in fairseq2.nn.transformer)": [[16, "fairseq2.nn.transformer.TransformerDecoder"]], "forward() (fairseq2.nn.transformer.transformerdecoder method)": [[16, "fairseq2.nn.transformer.TransformerDecoder.forward"]], "transformerdecoderlayer (class in fairseq2.nn.transformer)": [[17, "fairseq2.nn.transformer.TransformerDecoderLayer"]], "forward() (fairseq2.nn.transformer.transformerdecoderlayer method)": [[17, "fairseq2.nn.transformer.TransformerDecoderLayer.forward"]], "transformerencoder (class in fairseq2.nn.transformer)": [[18, "fairseq2.nn.transformer.TransformerEncoder"]], "forward() (fairseq2.nn.transformer.transformerencoder method)": [[18, "fairseq2.nn.transformer.TransformerEncoder.forward"]], "transformerencoderlayer (class in fairseq2.nn.transformer)": [[19, "fairseq2.nn.transformer.TransformerEncoderLayer"]], "forward() (fairseq2.nn.transformer.transformerencoderlayer method)": [[19, "fairseq2.nn.transformer.TransformerEncoderLayer.forward"]], "embedding (class in fairseq2.nn)": [[20, "fairseq2.nn.Embedding"]], "forward() (fairseq2.nn.embedding method)": [[20, "fairseq2.nn.Embedding.forward"]], "reset_parameters() (fairseq2.nn.embedding method)": [[20, "fairseq2.nn.Embedding.reset_parameters"]], "incrementalstate (class in fairseq2.nn)": [[21, "fairseq2.nn.IncrementalState"]], "reorder() (fairseq2.nn.incrementalstate method)": [[21, "fairseq2.nn.IncrementalState.reorder"]], "incrementalstatebag (class in fairseq2.nn)": [[22, "fairseq2.nn.IncrementalStateBag"]], "get_state() (fairseq2.nn.incrementalstatebag method)": [[22, "fairseq2.nn.IncrementalStateBag.get_state"]], "increment_step() (fairseq2.nn.incrementalstatebag method)": [[22, "fairseq2.nn.IncrementalStateBag.increment_step"]], "reorder() (fairseq2.nn.incrementalstatebag method)": [[22, "fairseq2.nn.IncrementalStateBag.reorder"]], "set_state() (fairseq2.nn.incrementalstatebag method)": [[22, "fairseq2.nn.IncrementalStateBag.set_state"]], "step (fairseq2.nn.incrementalstatebag property)": [[22, "fairseq2.nn.IncrementalStateBag.step"]], "learnedpositionencoder (class in fairseq2.nn)": [[23, "fairseq2.nn.LearnedPositionEncoder"]], "forward() (fairseq2.nn.learnedpositionencoder method)": [[23, "fairseq2.nn.LearnedPositionEncoder.forward"]], "reset_parameters() (fairseq2.nn.learnedpositionencoder method)": [[23, "fairseq2.nn.LearnedPositionEncoder.reset_parameters"]], "linear (class in fairseq2.nn)": [[24, "fairseq2.nn.Linear"]], "forward() (fairseq2.nn.linear method)": [[24, "fairseq2.nn.Linear.forward"]], "reset_parameters() (fairseq2.nn.linear method)": [[24, "fairseq2.nn.Linear.reset_parameters"]], "modulelist (class in fairseq2.nn)": [[25, "fairseq2.nn.ModuleList"]], "append() (fairseq2.nn.modulelist method)": [[25, "fairseq2.nn.ModuleList.append"]], "drop_iter() (fairseq2.nn.modulelist method)": [[25, "fairseq2.nn.ModuleList.drop_iter"]], "extend() (fairseq2.nn.modulelist method)": [[25, "fairseq2.nn.ModuleList.extend"]], "insert() (fairseq2.nn.modulelist method)": [[25, "fairseq2.nn.ModuleList.insert"]], "rotaryencoder (class in fairseq2.nn)": [[26, "fairseq2.nn.RotaryEncoder"]], "forward() (fairseq2.nn.rotaryencoder method)": [[26, "fairseq2.nn.RotaryEncoder.forward"]], "reset_non_persistent_buffers() (fairseq2.nn.rotaryencoder method)": [[26, "fairseq2.nn.RotaryEncoder.reset_non_persistent_buffers"]], "reset_parameters() (fairseq2.nn.rotaryencoder method)": [[26, "fairseq2.nn.RotaryEncoder.reset_parameters"]], "sinusoidalpositionencoder (class in fairseq2.nn)": [[27, "fairseq2.nn.SinusoidalPositionEncoder"]], "forward() (fairseq2.nn.sinusoidalpositionencoder method)": [[27, "fairseq2.nn.SinusoidalPositionEncoder.forward"]], "reset_non_persistent_buffers() (fairseq2.nn.sinusoidalpositionencoder method)": [[27, "fairseq2.nn.SinusoidalPositionEncoder.reset_non_persistent_buffers"]], "reset_parameters() (fairseq2.nn.sinusoidalpositionencoder method)": [[27, "fairseq2.nn.SinusoidalPositionEncoder.reset_parameters"]], "tiedprojection (class in fairseq2.nn)": [[28, "fairseq2.nn.TiedProjection"]], "forward() (fairseq2.nn.tiedprojection method)": [[28, "fairseq2.nn.TiedProjection.forward"]], "alibiattentionmaskgenerator (class in fairseq2.nn.transformer)": [[29, "fairseq2.nn.transformer.ALiBiAttentionMaskGenerator"]], "__call__() (fairseq2.nn.transformer.alibiattentionmaskgenerator method)": [[29, "fairseq2.nn.transformer.ALiBiAttentionMaskGenerator.__call__"]], "causalattentionmaskgenerator (class in fairseq2.nn.transformer)": [[30, "fairseq2.nn.transformer.CausalAttentionMaskGenerator"]], "__call__() (fairseq2.nn.transformer.causalattentionmaskgenerator method)": [[30, "fairseq2.nn.transformer.CausalAttentionMaskGenerator.__call__"]], "multiheadattentionstate (class in fairseq2.nn.transformer)": [[31, "fairseq2.nn.transformer.MultiheadAttentionState"]], "append() (fairseq2.nn.transformer.multiheadattentionstate method)": [[31, "fairseq2.nn.transformer.MultiheadAttentionState.append"]], "prev_k (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[31, "fairseq2.nn.transformer.MultiheadAttentionState.prev_k"]], "prev_key_padding_mask (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[31, "fairseq2.nn.transformer.MultiheadAttentionState.prev_key_padding_mask"]], "prev_v (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[31, "fairseq2.nn.transformer.MultiheadAttentionState.prev_v"]], "reorder() (fairseq2.nn.transformer.multiheadattentionstate method)": [[31, "fairseq2.nn.transformer.MultiheadAttentionState.reorder"]], "relativepositionsdpa (class in fairseq2.nn.transformer)": [[32, "fairseq2.nn.transformer.RelativePositionSDPA"]], "forward() (fairseq2.nn.transformer.relativepositionsdpa method)": [[32, "fairseq2.nn.transformer.RelativePositionSDPA.forward"]], "reset_parameters() (fairseq2.nn.transformer.relativepositionsdpa method)": [[32, "fairseq2.nn.transformer.RelativePositionSDPA.reset_parameters"]], "standardfeedforwardnetwork (class in fairseq2.nn.transformer)": [[33, "fairseq2.nn.transformer.StandardFeedForwardNetwork"]], "forward() (fairseq2.nn.transformer.standardfeedforwardnetwork method)": [[33, "fairseq2.nn.transformer.StandardFeedForwardNetwork.forward"]], "standardmultiheadattention (class in fairseq2.nn.transformer)": [[34, "fairseq2.nn.transformer.StandardMultiheadAttention"]], "_run_attn_weight_hooks() (fairseq2.nn.transformer.standardmultiheadattention method)": [[34, "fairseq2.nn.transformer.StandardMultiheadAttention._run_attn_weight_hooks"]], "forward() (fairseq2.nn.transformer.standardmultiheadattention method)": [[34, "fairseq2.nn.transformer.StandardMultiheadAttention.forward"]], "register_attn_weight_hook() (fairseq2.nn.transformer.standardmultiheadattention method)": [[34, "fairseq2.nn.transformer.StandardMultiheadAttention.register_attn_weight_hook"]], "reset_parameters() (fairseq2.nn.transformer.standardmultiheadattention method)": [[34, "fairseq2.nn.transformer.StandardMultiheadAttention.reset_parameters"]], "standardtransformerdecoder (class in fairseq2.nn.transformer)": [[35, "fairseq2.nn.transformer.StandardTransformerDecoder"]], "forward() (fairseq2.nn.transformer.standardtransformerdecoder method)": [[35, "fairseq2.nn.transformer.StandardTransformerDecoder.forward"]], "standardtransformerdecoderlayer (class in fairseq2.nn.transformer)": [[36, "fairseq2.nn.transformer.StandardTransformerDecoderLayer"]], "forward() (fairseq2.nn.transformer.standardtransformerdecoderlayer method)": [[36, "fairseq2.nn.transformer.StandardTransformerDecoderLayer.forward"]], "reset_parameters() (fairseq2.nn.transformer.standardtransformerdecoderlayer method)": [[36, "fairseq2.nn.transformer.StandardTransformerDecoderLayer.reset_parameters"]], "standardtransformerencoder (class in fairseq2.nn.transformer)": [[37, "fairseq2.nn.transformer.StandardTransformerEncoder"]], "forward() (fairseq2.nn.transformer.standardtransformerencoder method)": [[37, "fairseq2.nn.transformer.StandardTransformerEncoder.forward"]], "standardtransformerencoderlayer (class in fairseq2.nn.transformer)": [[38, "fairseq2.nn.transformer.StandardTransformerEncoderLayer"]], "forward() (fairseq2.nn.transformer.standardtransformerencoderlayer method)": [[38, "fairseq2.nn.transformer.StandardTransformerEncoderLayer.forward"]], "reset_parameters() (fairseq2.nn.transformer.standardtransformerencoderlayer method)": [[38, "fairseq2.nn.transformer.StandardTransformerEncoderLayer.reset_parameters"]], "storeattentionweights (class in fairseq2.nn.transformer)": [[39, "fairseq2.nn.transformer.StoreAttentionWeights"]], "__call__() (fairseq2.nn.transformer.storeattentionweights method)": [[39, "fairseq2.nn.transformer.StoreAttentionWeights.__call__"]], "cosineannealinglr (class in fairseq2.optim.lr_scheduler)": [[40, "fairseq2.optim.lr_scheduler.CosineAnnealingLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[40, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[40, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[40, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[40, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.state_dict"]], "lrschedulerbase (class in fairseq2.optim.lr_scheduler)": [[41, "fairseq2.optim.lr_scheduler.LRSchedulerBase"]], "get_last_lr() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[41, "fairseq2.optim.lr_scheduler.LRSchedulerBase.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[41, "fairseq2.optim.lr_scheduler.LRSchedulerBase.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[41, "fairseq2.optim.lr_scheduler.LRSchedulerBase.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[41, "fairseq2.optim.lr_scheduler.LRSchedulerBase.state_dict"]], "mylelr (class in fairseq2.optim.lr_scheduler)": [[42, "fairseq2.optim.lr_scheduler.MyleLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.mylelr method)": [[42, "fairseq2.optim.lr_scheduler.MyleLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.mylelr method)": [[42, "fairseq2.optim.lr_scheduler.MyleLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.mylelr method)": [[42, "fairseq2.optim.lr_scheduler.MyleLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.mylelr method)": [[42, "fairseq2.optim.lr_scheduler.MyleLR.state_dict"]], "noamlr (class in fairseq2.optim.lr_scheduler)": [[43, "fairseq2.optim.lr_scheduler.NoamLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.noamlr method)": [[43, "fairseq2.optim.lr_scheduler.NoamLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.noamlr method)": [[43, "fairseq2.optim.lr_scheduler.NoamLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.noamlr method)": [[43, "fairseq2.optim.lr_scheduler.NoamLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.noamlr method)": [[43, "fairseq2.optim.lr_scheduler.NoamLR.state_dict"]], "polynomialdecaylr (class in fairseq2.optim.lr_scheduler)": [[44, "fairseq2.optim.lr_scheduler.PolynomialDecayLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[44, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[44, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[44, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[44, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.state_dict"]], "post (fairseq2.nn.transformer.transformernormorder attribute)": [[45, "fairseq2.nn.transformer.TransformerNormOrder.POST"]], "pre (fairseq2.nn.transformer.transformernormorder attribute)": [[45, "fairseq2.nn.transformer.TransformerNormOrder.PRE"]], "pre_with_normformer (fairseq2.nn.transformer.transformernormorder attribute)": [[45, "fairseq2.nn.transformer.TransformerNormOrder.PRE_WITH_NORMFORMER"]], "transformernormorder (class in fairseq2.nn.transformer)": [[45, "fairseq2.nn.transformer.TransformerNormOrder"]], "__iter__() (fairseq2.nn.transformer.transformernormorder class method)": [[45, "fairseq2.nn.transformer.TransformerNormOrder.__iter__"]], "to_float_mask() (in module fairseq2.nn.utils.mask)": [[46, "fairseq2.nn.utils.mask.to_float_mask"]], "to_padding_mask() (in module fairseq2.nn.utils.mask)": [[47, "fairseq2.nn.utils.mask.to_padding_mask"]]}})