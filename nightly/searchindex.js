Search.setIndex({"docnames": ["bibliography", "index", "reference/abc", "reference/all", "reference/classes", "reference/enums", "reference/functions", "reference/generated/abc/fairseq2.gang.Gang", "reference/generated/abc/fairseq2.nn.PositionEncoder", "reference/generated/abc/fairseq2.nn.Projection", "reference/generated/abc/fairseq2.nn.transformer.AttentionMaskGenerator", "reference/generated/abc/fairseq2.nn.transformer.AttentionWeightHook", "reference/generated/abc/fairseq2.nn.transformer.FeedForwardNetwork", "reference/generated/abc/fairseq2.nn.transformer.MultiheadAttention", "reference/generated/abc/fairseq2.nn.transformer.SDPA", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoder", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoderLayer", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoder", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoderLayer", "reference/generated/classes/fairseq2.nn.Embedding", "reference/generated/classes/fairseq2.nn.IncrementalState", "reference/generated/classes/fairseq2.nn.IncrementalStateBag", "reference/generated/classes/fairseq2.nn.LearnedPositionEncoder", "reference/generated/classes/fairseq2.nn.Linear", "reference/generated/classes/fairseq2.nn.ModuleList", "reference/generated/classes/fairseq2.nn.RotaryEncoder", "reference/generated/classes/fairseq2.nn.SinusoidalPositionEncoder", "reference/generated/classes/fairseq2.nn.TiedProjection", "reference/generated/classes/fairseq2.nn.transformer.ALiBiAttentionMaskGenerator", "reference/generated/classes/fairseq2.nn.transformer.CausalAttentionMaskGenerator", "reference/generated/classes/fairseq2.nn.transformer.MultiheadAttentionState", "reference/generated/classes/fairseq2.nn.transformer.RelativePositionSDPA", "reference/generated/classes/fairseq2.nn.transformer.StandardFeedForwardNetwork", "reference/generated/classes/fairseq2.nn.transformer.StandardMultiheadAttention", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoder", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoderLayer", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoder", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoderLayer", "reference/generated/classes/fairseq2.nn.transformer.StoreAttentionWeights", "reference/generated/classes/fairseq2.optim.lr_scheduler.CosineAnnealingLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.LRSchedulerBase", "reference/generated/classes/fairseq2.optim.lr_scheduler.MyleLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.NoamLR", "reference/generated/classes/fairseq2.optim.lr_scheduler.PolynomialDecayLR", "reference/generated/enums/fairseq2.nn.transformer.TransformerNormOrder", "reference/generated/functions/fairseq2.nn.utils.mask.to_float_mask", "reference/generated/functions/fairseq2.nn.utils.mask.to_padding_mask"], "filenames": ["bibliography.rst", "index.rst", "reference/abc.rst", "reference/all.rst", "reference/classes.rst", "reference/enums.rst", "reference/functions.rst", "reference/generated/abc/fairseq2.gang.Gang.rst", "reference/generated/abc/fairseq2.nn.PositionEncoder.rst", "reference/generated/abc/fairseq2.nn.Projection.rst", "reference/generated/abc/fairseq2.nn.transformer.AttentionMaskGenerator.rst", "reference/generated/abc/fairseq2.nn.transformer.AttentionWeightHook.rst", "reference/generated/abc/fairseq2.nn.transformer.FeedForwardNetwork.rst", "reference/generated/abc/fairseq2.nn.transformer.MultiheadAttention.rst", "reference/generated/abc/fairseq2.nn.transformer.SDPA.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoder.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerDecoderLayer.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoder.rst", "reference/generated/abc/fairseq2.nn.transformer.TransformerEncoderLayer.rst", "reference/generated/classes/fairseq2.nn.Embedding.rst", "reference/generated/classes/fairseq2.nn.IncrementalState.rst", "reference/generated/classes/fairseq2.nn.IncrementalStateBag.rst", "reference/generated/classes/fairseq2.nn.LearnedPositionEncoder.rst", "reference/generated/classes/fairseq2.nn.Linear.rst", "reference/generated/classes/fairseq2.nn.ModuleList.rst", "reference/generated/classes/fairseq2.nn.RotaryEncoder.rst", "reference/generated/classes/fairseq2.nn.SinusoidalPositionEncoder.rst", "reference/generated/classes/fairseq2.nn.TiedProjection.rst", "reference/generated/classes/fairseq2.nn.transformer.ALiBiAttentionMaskGenerator.rst", "reference/generated/classes/fairseq2.nn.transformer.CausalAttentionMaskGenerator.rst", "reference/generated/classes/fairseq2.nn.transformer.MultiheadAttentionState.rst", "reference/generated/classes/fairseq2.nn.transformer.RelativePositionSDPA.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardFeedForwardNetwork.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardMultiheadAttention.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoder.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerDecoderLayer.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoder.rst", "reference/generated/classes/fairseq2.nn.transformer.StandardTransformerEncoderLayer.rst", "reference/generated/classes/fairseq2.nn.transformer.StoreAttentionWeights.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.CosineAnnealingLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.LRSchedulerBase.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.MyleLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.NoamLR.rst", "reference/generated/classes/fairseq2.optim.lr_scheduler.PolynomialDecayLR.rst", "reference/generated/enums/fairseq2.nn.transformer.TransformerNormOrder.rst", "reference/generated/functions/fairseq2.nn.utils.mask.to_float_mask.rst", "reference/generated/functions/fairseq2.nn.utils.mask.to_padding_mask.rst"], "titles": ["Bibliography", "fairseq2 documentation", "ABCs and Protocols", "All", "Classes", "Enums", "Functions", "Gang", "PositionEncoder", "Projection", "AttentionMaskGenerator", "AttentionWeightHook", "FeedForwardNetwork", "MultiheadAttention", "SDPA", "TransformerDecoder", "TransformerDecoderLayer", "TransformerEncoder", "TransformerEncoderLayer", "Embedding", "IncrementalState", "IncrementalStateBag", "LearnedPositionEncoder", "Linear", "ModuleList", "RotaryEncoder", "SinusoidalPositionEncoder", "TiedProjection", "ALiBiAttentionMaskGenerator", "CausalAttentionMaskGenerator", "MultiheadAttentionState", "RelativePositionSDPA", "StandardFeedForwardNetwork", "StandardMultiheadAttention", "StandardTransformerDecoder", "StandardTransformerDecoderLayer", "StandardTransformerEncoder", "StandardTransformerEncoderLayer", "StoreAttentionWeights", "CosineAnnealingLR", "LRSchedulerBase", "MyleLR", "NoamLR", "PolynomialDecayLR", "TransformerNormOrder", "to_float_mask", "to_padding_mask"], "terms": {"altdj": [0, 33], "23": [0, 33], "joshua": 0, "ainsli": [0, 33], "jame": 0, "lee": 0, "thorp": 0, "michiel": 0, "de": 0, "jong": 0, "yuri": 0, "zemlyanskii": 0, "federico": 0, "lebr\u00f3n": 0, "sumit": 0, "sanghai": 0, "gqa": 0, "train": [0, 1, 19, 24, 39, 41, 42, 43], "gener": [0, 1, 10, 28, 29, 34], "multi": [0, 13, 33], "queri": [0, 13, 14, 31, 33], "transform": [0, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 23, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 44], "model": [0, 1, 10, 12, 13, 15, 16, 17, 18, 26, 28, 29, 31, 32, 33, 34, 35, 36, 37, 42], "from": [0, 7, 19, 22, 23, 24, 26, 29, 30, 39, 40, 41, 42, 43], "head": [0, 13, 28, 30, 31, 33], "checkpoint": 0, "2023": 0, "arxiv": 0, "2305": 0, "13245": 0, "dyi": [0, 31], "19": [0, 31], "zihang": 0, "dai": [0, 31], "zhilin": 0, "yang": 0, "yime": 0, "jaim": 0, "carbonel": 0, "quoc": 0, "v": [0, 11, 13, 14, 30, 31, 33], "le": 0, "ruslan": 0, "salakhutdinov": 0, "xl": 0, "attent": [0, 10, 11, 13, 14, 15, 16, 28, 29, 30, 31, 33, 34, 35, 37, 38], "languag": [0, 1], "beyond": 0, "fix": [0, 19, 26], "length": [0, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 46], "context": 0, "2019": 0, "1901": 0, "02860": 0, "fgj19": [0, 34, 36], "angela": 0, "fan": [0, 34, 36], "edouard": 0, "grave": 0, "armand": 0, "joulin": 0, "reduc": [0, 7], "depth": 0, "demand": 0, "structur": 0, "dropout": [0, 14, 31, 32, 35, 37], "url": 0, "http": 0, "org": 0, "ab": 0, "1909": 0, "11556": 0, "doi": 0, "10": [0, 29], "48550": 0, "lh17": [0, 39], "ilya": 0, "loshchilov": [0, 39], "frank": 0, "hutter": [0, 39], "sgdr": 0, "stochast": 0, "gradient": [0, 19], "descent": 0, "warm": 0, "restart": [0, 39], "2017": 0, "1608": 0, "03983": 0, "psl21": [0, 28], "ofir": 0, "press": [0, 28], "noah": 0, "A": [0, 13, 20, 30, 33], "smith": 0, "mike": 0, "lewi": 0, "short": 0, "test": 0, "long": [0, 20], "linear": [0, 9, 27], "bias": 0, "enabl": 0, "input": [0, 7, 8, 9, 14, 19, 20, 22, 23, 25, 26, 27, 31], "extrapol": 0, "2021": 0, "2108": 0, "12409": 0, "swo21": [0, 33, 35, 37, 44], "sam": 0, "shleifer": [0, 33, 35, 37, 44], "jason": 0, "weston": 0, "myle": [0, 41], "ott": [0, 41], "normform": 0, "improv": 0, "pretrain": 0, "extra": [0, 24], "normal": [0, 32, 34, 35, 36, 37, 44], "2110": 0, "09456": 0, "slp": [0, 25], "21": [0, 25], "jianlin": 0, "su": [0, 25], "yu": 0, "lu": 0, "shengfeng": 0, "pan": 0, "ahm": 0, "murtadha": 0, "bo": 0, "wen": 0, "yunfeng": 0, "liu": 0, "roform": 0, "enhanc": 0, "rotari": 0, "posit": [0, 8, 13, 21, 22, 25, 26, 31, 33], "embed": [0, 22], "2104": 0, "09864": 0, "vsp": [0, 26, 32, 33, 34, 35, 36, 37, 42, 44], "17": [0, 26, 32, 33, 34, 35, 36, 37, 42, 44], "ashish": 0, "vaswani": [0, 26, 32, 33, 34, 35, 36, 37, 42, 44], "noam": [0, 41, 42], "shazeer": [0, 42], "niki": 0, "parmar": 0, "jakob": 0, "uszkoreit": 0, "llion": 0, "jone": 0, "aidan": 0, "n": [0, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 46], "gomez": 0, "lukasz": 0, "kaiser": 0, "illia": 0, "polosukhin": 0, "i": [0, 1, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 46], "all": [0, 1, 7, 9, 21, 23, 24, 27, 39, 41, 43], "you": 0, "need": [0, 20], "1706": 0, "03762": 0, "xyh": [0, 44], "20": [0, 44], "ruibin": 0, "xiong": [0, 44], "yunchang": 0, "di": 0, "he": 0, "kai": 0, "zheng": 0, "shuxin": 0, "chen": 0, "xing": 0, "huishuai": 0, "zhang": 0, "yanyan": 0, "lan": 0, "liwei": 0, "wang": 0, "tie": 0, "yan": 0, "On": 0, "layer": [0, 13, 15, 16, 17, 18, 24, 32, 34, 35, 36, 37, 44], "architectur": 0, "2020": 0, "2002": 0, "04745": 0, "sequenc": [1, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 21, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 43, 46], "toolkit": 1, "allow": 1, "research": 1, "develop": 1, "custom": 1, "translat": 1, "summar": 1, "other": 1, "content": 1, "task": 1, "abc": [1, 7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 20, 40], "protocol": [1, 10, 11, 28, 29, 38], "class": [1, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], "enum": [1, 44], "function": [1, 38], "bibliographi": 1, "fairseq2": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], "rank": 7, "size": [7, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 46], "devic": [7, 19, 22, 23, 25, 26, 31, 32, 33, 34, 35, 36, 37], "sourc": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46], "base": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44], "repres": [7, 11, 12, 13, 15, 16, 17, 18, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 46], "set": [7, 21, 24, 33], "process": [7, 16, 18, 35, 37], "work": 7, "collect": 7, "paramet": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46], "int": [7, 8, 9, 12, 13, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 30, 31, 32, 33, 39, 41, 42, 43], "The": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 41, 42, 43, 45, 46], "thi": [7, 20, 21, 23, 24, 26, 28, 29, 30, 38, 39, 41, 42, 43], "number": [7, 8, 13, 14, 22, 24, 25, 26, 28, 30, 31, 33, 39, 41, 42, 43, 46], "ar": [7, 9, 23, 26, 27], "part": [7, 29], "associ": [7, 39, 41, 42, 43], "abstract": [7, 8, 9, 12, 13, 14, 15, 16, 17, 18, 20, 40], "all_gath": 7, "output_tensor": 7, "input_tensor": 7, "gather": 7, "tensor": [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 45, 46], "put": 7, "them": [7, 35, 37], "singl": 7, "output": [7, 9, 15, 16, 17, 18, 20, 23, 27, 32, 33, 34, 35, 36, 37], "accomod": 7, "element": [7, 46], "current": [7, 21, 30, 39, 40, 41, 42, 43], "all_reduc": 7, "op": 7, "across": 7, "oper": 7, "reduceoper": 7, "wise": 7, "as_process_group": 7, "return": [7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46], "group": [7, 33, 39, 40, 41, 42, 43], "type": [7, 8, 9, 10, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 44, 45, 46], "processgroup": 7, "barrier": 7, "synchron": 7, "nn": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 44, 45, 46], "encoding_dim": [8, 22, 25, 26], "max_seq_len": [8, 22, 25, 26], "modul": [8, 9, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 30, 31, 32, 33, 34, 35, 36, 37, 44], "encod": [8, 15, 16, 17, 18, 22, 25, 26, 31, 33, 34, 35, 36, 37], "inform": [8, 21, 22, 25, 26, 35, 37], "dimension": [8, 9, 10, 12, 13, 15, 16, 17, 18, 19, 22, 23, 25, 26, 27, 28, 29, 31, 32, 33, 34, 35, 36, 37, 42], "none": [8, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 26, 27, 30, 31, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 44, 45, 46], "expect": [8, 21, 22, 25, 26], "maximum": [8, 22, 25, 26], "_do_forward": 8, "seq": [8, 10, 12, 15, 16, 17, 18, 22, 25, 26, 28, 29, 32, 34, 35, 36, 37, 46], "padding_mask": [8, 13, 15, 16, 17, 18, 22, 25, 26, 33, 34, 35, 36, 37], "state_bag": [8, 13, 15, 16, 22, 25, 26, 33, 34, 35], "shape": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 45, 46], "": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 44, 46], "e": [8, 19, 21, 22, 25, 26, 30, 33, 39], "where": [8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 39, 46], "ani": [8, 14, 19, 20, 22, 24, 25, 26, 31, 45, 46], "batch": [8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 20, 22, 25, 26, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 46], "dimens": [8, 9, 14, 22, 23, 25, 26, 27, 31, 46], "includ": [8, 14, 22, 25, 26, 31, 43, 46], "float": [8, 13, 14, 15, 16, 17, 18, 22, 24, 25, 26, 30, 31, 32, 33, 34, 35, 36, 37, 39, 41, 43, 45, 46], "pad": [8, 13, 15, 16, 17, 18, 22, 25, 26, 30, 33, 34, 35, 36, 37, 46], "mask": [8, 10, 13, 14, 15, 16, 17, 18, 22, 25, 26, 28, 29, 30, 31, 33, 34, 35, 36, 37, 45, 46], "incrementalstatebag": [8, 13, 15, 16, 22, 25, 26, 33, 34, 35], "state": [8, 13, 15, 16, 20, 21, 22, 25, 26, 30, 33, 34, 35, 39, 40, 41, 42, 43], "bag": [8, 13, 15, 16, 21, 22, 25, 26, 33, 34, 35], "us": [8, 13, 15, 16, 20, 21, 22, 23, 25, 26, 27, 30, 32, 33, 34, 35, 36, 37, 42, 43], "increment": [8, 13, 15, 16, 20, 21, 22, 25, 26, 30, 33, 34, 35], "evalu": [8, 13, 15, 16, 20, 21, 22, 25, 26, 30, 33, 34, 35], "same": [8, 9, 12, 14, 15, 16, 17, 18, 22, 23, 25, 26, 27, 31, 32, 34, 35, 36, 37, 46], "forward": [8, 9, 12, 13, 14, 15, 16, 17, 18, 19, 22, 23, 25, 26, 27, 31, 32, 33, 34, 35, 36, 37], "input_dim": [9, 23], "output_dim": [9, 23], "appli": [9, 23, 27, 32, 33, 34, 36, 44], "incom": [9, 23, 27], "data": [9, 23, 27], "x": [9, 19, 23, 24, 27], "h_": [9, 23, 27], "inp": [9, 23, 27], "out": [9, 23, 27], "last": [9, 23, 27, 39, 40, 41, 42, 43], "arg": [10, 11, 24, 39, 40, 41, 42, 43], "kwarg": [10, 11], "an": [10, 13, 20, 21, 23, 24, 28, 29, 30, 32, 33, 34, 39, 40, 41, 42, 43, 46], "__call__": [10, 11, 28, 29, 38], "which": [10, 13, 21, 26, 28, 29, 33, 38, 39, 40, 41, 42, 43], "m": [10, 11, 12, 13, 15, 16, 17, 18, 21, 22, 26, 28, 29, 32, 33, 34, 35, 36, 37, 38], "implement": [10, 33, 39, 41], "defin": 10, "specif": [10, 46], "hook": [11, 13, 33], "pass": 11, "register_attn_weight_hook": [11, 13, 33], "attn": [11, 13, 33, 38], "attn_weight": [11, 13, 33, 38], "multiheadattent": [11, 30, 33, 35, 37], "ha": [11, 20, 30, 39], "comput": [11, 13, 14, 16, 30, 31, 33, 35, 39, 40, 41, 42, 43], "weight": [11, 13, 14, 16, 23, 27, 31, 33, 35, 38], "valu": [11, 13, 14, 21, 30, 31, 33, 39, 44], "s_": [11, 13, 14, 15, 16, 30, 31, 33, 34, 35], "kv": [11, 13, 14, 31, 33], "kei": [11, 13, 14, 30, 31, 33], "model_dim": [12, 13, 15, 16, 17, 18, 31, 32, 33], "feed": [12, 32, 35, 37], "network": [12, 32, 35, 37], "project": [12, 23, 27, 30, 32, 33], "num_head": [13, 28, 31, 33], "_run_attn_weight_hook": [13, 33], "run": [13, 33], "regist": [13, 33], "attn_mask": [13, 33], "key_padding_mask": [13, 30, 33], "k": [13, 14, 23, 30, 31, 33], "ad": [13, 14, 16, 31, 33, 35, 37], "befor": [13, 14, 16, 24, 31, 33, 35, 37, 39], "indic": [13, 19, 33], "ignor": [13, 33], "purpos": [13, 33], "call": [13, 15, 17, 20, 21, 30, 33, 34, 36, 38, 39, 40, 41, 42, 43], "everi": [13, 21, 24, 33, 39, 40, 41, 42, 43], "time": [13, 33], "after": [13, 21, 33, 39, 42, 43, 44], "attentionweighthook": [13, 33, 38], "handl": [13, 33], "can": [13, 33], "remov": [13, 33], "removablehandl": [13, 33], "attn_dropout_p": [14, 31], "0": [14, 19, 22, 24, 26, 29, 31, 32, 34, 35, 36, 37, 39, 41, 43, 44], "scale": [14, 19, 31, 33, 35, 37, 39, 41], "dot": [14, 31, 33], "product": [14, 31, 33], "probabl": [14, 24, 31, 32, 35, 37], "needs_weight": [14, 31], "fals": [14, 19, 23, 31, 33, 35, 37, 39, 40, 41, 42, 43], "bool": [14, 19, 23, 31, 32, 33, 35, 37, 39, 41, 42, 43], "If": [14, 15, 17, 19, 21, 23, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43], "true": [14, 19, 23, 31, 32, 33, 35, 37, 39, 41, 42, 43], "tupl": [14, 15, 16, 17, 18, 30, 31, 34, 35, 36, 37, 38], "decod": [15, 16, 34, 35], "encoder_output": [15, 16, 34, 35], "encoder_padding_mask": [15, 16, 34, 35], "layer_output_hook": [15, 17, 34, 36], "enc": [15, 16, 34, 35], "m_": [15, 16, 34, 35], "encoder_out": [15, 34], "decoderlayeroutputhook": [15, 34], "each": [15, 17, 34, 36, 39, 41, 42, 43, 44, 46], "stack": [15, 17, 34, 36], "self_attn_mask": [16, 35], "self": [16, 28, 29, 35, 37, 38, 39, 40, 41, 42, 43], "encoderlayeroutputhook": [17, 36], "final": [19, 22, 24, 25, 26, 27, 31, 32, 33, 34, 35, 36, 37, 39, 41, 42, 43], "num_embed": 19, "embedding_dim": 19, "pad_idx": 19, "dtype": [19, 22, 23, 31, 32, 33, 34, 35, 36, 37, 45], "store": [19, 21, 38], "dictionari": 19, "tabl": [19, 31], "entri": [19, 39, 40, 41, 42, 43], "do": 19, "contribut": 19, "therefor": 19, "updat": [19, 39, 41, 42, 43], "dure": [19, 20, 21, 24, 30, 39, 43], "initi": [19, 23, 26, 39, 41, 43], "mathcal": [19, 23], "frac": [19, 23, 26, 39, 41, 42, 43], "1": [19, 21, 22, 23, 24, 26, 30, 33, 35, 37, 39, 40, 41, 42, 43, 44], "text": [19, 23, 26, 39], "otherwis": 19, "correspond": [19, 20, 41, 42, 43], "specifi": [19, 30, 33, 44], "reset_paramet": [19, 22, 23, 25, 26, 31, 33, 35, 37], "reset": [19, 22, 23, 25, 26, 31, 33, 35, 37], "buffer": [19, 22, 23, 25, 26, 31, 33, 35, 37], "hold": [20, 21, 24, 30], "special": 20, "mode": 20, "onli": 20, "receiv": 20, "previou": 20, "must": 20, "produc": [20, 33], "next": 20, "thu": 20, "cach": 20, "term": 20, "about": [20, 39], "reorder": [20, 21, 30], "new_ord": [20, 21, 30], "rearrang": [20, 30], "accord": [20, 30], "new": [20, 30], "order": [20, 30, 32, 34, 35, 36, 37, 44], "when": [20, 30], "chang": [20, 30], "typic": [20, 30], "case": [20, 30], "beam": [20, 21, 30], "search": [20, 21, 30], "between": [20, 30], "step": [20, 21, 30, 33, 39, 41, 42, 43], "select": [20, 30], "It": [20, 21, 30, 39, 40, 41, 42, 43], "frequent": [20, 30], "torch": [20, 22, 23, 24, 26, 29, 30], "index_select": [20, 30], "object": [21, 28, 29, 38, 39, 40, 41, 42, 43], "get_stat": 21, "kl": 21, "get": 21, "present": 21, "t": [21, 39, 41, 42, 43], "doe": 21, "match": 21, "increment_step": 21, "delta": 21, "method": [21, 23], "should": [21, 30, 39, 40, 41, 42, 43], "g": [21, 29, 30], "keep": 21, "track": 21, "see": [21, 26, 35, 37], "incrementalst": [21, 30], "more": [21, 26, 35, 37, 39], "set_stat": 21, "properti": 21, "positionencod": [22, 25, 26, 33], "learn": [22, 23, 32, 33, 39, 40, 41, 42, 43], "usag": [22, 24, 26, 29], "import": [22, 24, 26, 29], "position_encod": [22, 26], "4": [22, 26, 29], "16": [22, 26], "ones": [22, 26], "3": [22, 24, 26, 29, 42], "1135": 22, "5548": 22, "4293": 22, "2": [22, 26, 39, 44], "0112": 22, "po": [22, 26], "2364": 22, "6009": 22, "3865": 22, "4810": 22, "4746": 22, "4544": 22, "2761": 22, "8828": 22, "grad_fn": 22, "squeezebackward1": 22, "bia": [23, 27, 32, 33], "skip_init": 23, "unless": 23, "overridden": 23, "subclass": 23, "u": 23, "sqrt": [23, 41, 42], "ident": 23, "addit": [23, 32, 33, 39], "left": 23, "uniniti": 23, "becom": 23, "noop": 23, "intend": 23, "author": [23, 42], "who": 23, "want": 23, "differ": [23, 26], "drop_p": 24, "submodul": 24, "list": 24, "extend": [24, 33], "featur": 24, "option": 24, "drop": 24, "random": 24, "iter": 24, "layer1": 24, "layer2": 24, "layer3": 24, "5": [24, 26, 42], "drop_it": 24, "might": 24, "over": [24, 43], "add": 24, "append": [24, 30], "given": 24, "end": [24, 39], "python": 24, "insert": 24, "index": [24, 39, 41, 42, 43, 46], "rel": [25, 31, 39], "describ": [25, 28, 31, 32, 33, 34, 35, 36, 37, 39, 42, 44], "et": [25, 26, 28, 31, 32, 33, 34, 35, 36, 37, 42, 44], "al": [25, 26, 28, 31, 32, 33, 34, 35, 36, 37, 42, 44], "reset_non_persistent_buff": [25, 26], "non": [25, 26], "persist": [25, 26], "_legacy_pad_idx": 26, "sinusoid": 26, "tensor2tensor": 26, "slightli": 26, "descript": 26, "section": [26, 42], "mean": 26, "instead": 26, "pe_": 26, "2i": 26, "sin": 26, "10000": 26, "d_": 26, "co": [26, 39], "we": 26, "geq": 26, "here": 26, "0000e": 26, "00": 26, "9": 26, "4147e": 26, "01": 26, "04": 26, "6": 26, "4030e": 26, "0930e": 26, "02": 26, "1615e": 26, "anoth": 27, "instanc": [27, 34], "share": 27, "follow": [28, 29, 38], "attentionmaskgener": [28, 29, 34], "alibi": 28, "h": [28, 30], "causal": 29, "whose": 29, "upper": 29, "triangular": 29, "abov": 29, "main": 29, "diagon": 29, "fill": 29, "neg": 29, "infin": 29, "while": 29, "its": 29, "rest": 29, "zero": [29, 33, 34, 36], "empti": [29, 33], "inf": 29, "bootstrap": 30, "intern": 30, "stp": 30, "k_": 30, "proj": 30, "v_": 30, "cache_reserve_s": 30, "512": 30, "reserv": 30, "capac": 30, "increas": [30, 39, 41, 42, 43], "multipli": 30, "accumul": 30, "past": 30, "seq_len": [30, 46], "pos_encod": [31, 33], "sdpa": [31, 33], "param": 31, "inner_dim": 32, "inner_activ": 32, "inner_dropout_p": 32, "norm_ord": [32, 34, 35, 36, 37], "transformernormord": [32, 34, 35, 36, 37], "post": [32, 34, 35, 36, 37, 44], "layer_norm_fn": [32, 34, 35, 36, 37], "feedforwardnetwork": [32, 35, 37], "inner": 32, "both": 32, "activ": 32, "relu": 32, "layernormfactori": [32, 34, 35, 36, 37], "factori": [32, 34, 35, 36, 37], "construct": [32, 34, 35, 36, 37], "num_key_value_head": 33, "q_proj": 33, "k_proj": 33, "v_proj": 33, "add_bias_kv": 33, "add_zero_attn": 33, "scale_head": 33, "output_proj": 33, "equival": 33, "standard": 33, "mha": 33, "mqa": 33, "default": 33, "explicitli": 33, "self_attn_mask_gen": 34, "layer_drop_p": [34, 36], "transformerdecod": 34, "modulelist": [34, 36], "causalattentionmaskgener": 34, "greater": [34, 36], "than": [34, 36], "layerdrop": [34, 36], "self_attn": [35, 37], "encoder_decoder_attn": 35, "ffn": [35, 37], "scale_residu": [35, 37], "dropout_p": [35, 37], "transformerdecoderlay": 35, "residu": [35, 37, 44], "transformerencod": 36, "transformerencoderlay": 37, "storag": 38, "provid": 38, "mutablesequ": 38, "optim": [39, 40, 41, 42, 43], "lr_schedul": [39, 40, 41, 42, 43], "cycle_len": 39, "num_warmup_step": [39, 41, 42, 43], "cycle_mul": 39, "lr_mul": 39, "start_lr": [39, 41, 43], "final_lr": [39, 43], "last_epoch": [39, 40, 41, 42, 43], "verbos": [39, 40, 41, 42, 43], "lrschedulerbas": [39, 41, 42, 43], "rate": [39, 40, 41, 42, 43], "schedul": [39, 40, 41, 42, 43], "warmup": [39, 41, 42, 43], "eta_t": [39, 41, 42, 43], "eta_": [39, 41, 42, 43], "t_": [39, 41, 42, 43], "pi": 39, "anneal": 39, "cycl": 39, "t_i": 39, "taken": 39, "sinc": 39, "total": [39, 43], "within": 39, "th": 39, "cosin": 39, "effect": 39, "start": [39, 44], "larg": 39, "rapidli": 39, "decreas": [39, 41, 42, 43], "minimum": 39, "being": 39, "again": 39, "pleas": 39, "refer": [39, 42], "paper": [39, 42], "detail": 39, "In": [39, 42], "origin": [39, 41], "also": 39, "support": 39, "phase": 39, "linearli": [39, 41, 42, 43], "first": [39, 41, 42, 43], "chainabl": [39, 41, 42, 43], "factor": 39, "grow": 39, "respect": [39, 41, 43], "epoch": [39, 40, 41, 42, 43], "print": [39, 41, 42, 43], "messag": [39, 41, 42, 43], "stdout": [39, 41, 42, 43], "get_last_lr": [39, 40, 41, 42, 43], "load_state_dict": [39, 40, 41, 42, 43], "state_dict": [39, 40, 41, 42, 43], "load": [39, 40, 41, 42, 43], "dict": [39, 40, 41, 42, 43], "print_lr": [39, 40, 41, 42, 43], "is_verbos": [39, 40, 41, 42, 43], "lr": [39, 40, 41, 42, 43], "displai": [39, 40, 41, 42, 43], "contain": [39, 40, 41, 42, 43], "variabl": [39, 40, 41, 42, 43], "__dict__": [39, 40, 41, 42, 43], "_lrschedul": 40, "version": 41, "noamlr": 41, "preserv": 41, "min": [41, 42], "essenti": 41, "squar": [41, 42], "root": [41, 42], "wa": 41, "propos": 41, "fairseq": 41, "under": 41, "name": [41, 44], "inversesquarerootlr": 41, "thereaft": [41, 42, 43], "proportion": [41, 42], "invers": [41, 42], "commonli": 42, "second": 42, "num_step": 43, "power": 43, "polynomi": 43, "decai": 43, "p": 43, "degre": 43, "expon": 43, "qualnam": 44, "boundari": 44, "classmethod": 44, "__iter__": 44, "member": 44, "definit": 44, "connect": 44, "pre": 44, "begin": 44, "pre_with_normform": 44, "util": [45, 46], "convert": [45, 46], "boolean": 45, "point": 45, "arrai": 46}, "objects": {"fairseq2.gang": [[7, 0, 1, "", "Gang"]], "fairseq2.gang.Gang": [[7, 1, 1, "", "all_gather"], [7, 1, 1, "", "all_reduce"], [7, 1, 1, "", "as_process_group"], [7, 1, 1, "", "barrier"]], "fairseq2.nn": [[19, 0, 1, "", "Embedding"], [20, 0, 1, "", "IncrementalState"], [21, 0, 1, "", "IncrementalStateBag"], [22, 0, 1, "", "LearnedPositionEncoder"], [23, 0, 1, "", "Linear"], [24, 0, 1, "", "ModuleList"], [8, 0, 1, "", "PositionEncoder"], [9, 0, 1, "", "Projection"], [25, 0, 1, "", "RotaryEncoder"], [26, 0, 1, "", "SinusoidalPositionEncoder"], [27, 0, 1, "", "TiedProjection"]], "fairseq2.nn.Embedding": [[19, 1, 1, "", "forward"], [19, 1, 1, "", "reset_parameters"]], "fairseq2.nn.IncrementalState": [[20, 1, 1, "", "reorder"]], "fairseq2.nn.IncrementalStateBag": [[21, 1, 1, "", "get_state"], [21, 1, 1, "", "increment_step"], [21, 1, 1, "", "reorder"], [21, 1, 1, "", "set_state"], [21, 2, 1, "", "step"]], "fairseq2.nn.LearnedPositionEncoder": [[22, 1, 1, "", "forward"], [22, 1, 1, "", "reset_parameters"]], "fairseq2.nn.Linear": [[23, 1, 1, "", "forward"], [23, 1, 1, "", "reset_parameters"]], "fairseq2.nn.ModuleList": [[24, 1, 1, "", "append"], [24, 1, 1, "", "drop_iter"], [24, 1, 1, "", "extend"], [24, 1, 1, "", "insert"]], "fairseq2.nn.PositionEncoder": [[8, 1, 1, "", "_do_forward"], [8, 1, 1, "", "forward"]], "fairseq2.nn.Projection": [[9, 1, 1, "", "forward"]], "fairseq2.nn.RotaryEncoder": [[25, 1, 1, "", "forward"], [25, 1, 1, "", "reset_non_persistent_buffers"], [25, 1, 1, "", "reset_parameters"]], "fairseq2.nn.SinusoidalPositionEncoder": [[26, 1, 1, "", "forward"], [26, 1, 1, "", "reset_non_persistent_buffers"], [26, 1, 1, "", "reset_parameters"]], "fairseq2.nn.TiedProjection": [[27, 1, 1, "", "forward"]], "fairseq2.nn.transformer": [[28, 0, 1, "", "ALiBiAttentionMaskGenerator"], [10, 0, 1, "", "AttentionMaskGenerator"], [11, 0, 1, "", "AttentionWeightHook"], [29, 0, 1, "", "CausalAttentionMaskGenerator"], [12, 0, 1, "", "FeedForwardNetwork"], [13, 0, 1, "", "MultiheadAttention"], [30, 0, 1, "", "MultiheadAttentionState"], [31, 0, 1, "", "RelativePositionSDPA"], [14, 0, 1, "", "SDPA"], [32, 0, 1, "", "StandardFeedForwardNetwork"], [33, 0, 1, "", "StandardMultiheadAttention"], [34, 0, 1, "", "StandardTransformerDecoder"], [35, 0, 1, "", "StandardTransformerDecoderLayer"], [36, 0, 1, "", "StandardTransformerEncoder"], [37, 0, 1, "", "StandardTransformerEncoderLayer"], [38, 0, 1, "", "StoreAttentionWeights"], [15, 0, 1, "", "TransformerDecoder"], [16, 0, 1, "", "TransformerDecoderLayer"], [17, 0, 1, "", "TransformerEncoder"], [18, 0, 1, "", "TransformerEncoderLayer"], [44, 0, 1, "", "TransformerNormOrder"]], "fairseq2.nn.transformer.ALiBiAttentionMaskGenerator": [[28, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.AttentionMaskGenerator": [[10, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.AttentionWeightHook": [[11, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.CausalAttentionMaskGenerator": [[29, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.FeedForwardNetwork": [[12, 1, 1, "", "forward"]], "fairseq2.nn.transformer.MultiheadAttention": [[13, 1, 1, "", "_run_attn_weight_hooks"], [13, 1, 1, "", "forward"], [13, 1, 1, "", "register_attn_weight_hook"]], "fairseq2.nn.transformer.MultiheadAttentionState": [[30, 1, 1, "", "append"], [30, 3, 1, "", "cache_reserve_size"], [30, 3, 1, "", "k"], [30, 3, 1, "", "key_padding_mask"], [30, 1, 1, "", "reorder"], [30, 3, 1, "", "seq_len"], [30, 3, 1, "", "v"]], "fairseq2.nn.transformer.RelativePositionSDPA": [[31, 1, 1, "", "forward"], [31, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.SDPA": [[14, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardFeedForwardNetwork": [[32, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardMultiheadAttention": [[33, 1, 1, "", "_run_attn_weight_hooks"], [33, 1, 1, "", "forward"], [33, 1, 1, "", "register_attn_weight_hook"], [33, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.StandardTransformerDecoder": [[34, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardTransformerDecoderLayer": [[35, 1, 1, "", "forward"], [35, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.StandardTransformerEncoder": [[36, 1, 1, "", "forward"]], "fairseq2.nn.transformer.StandardTransformerEncoderLayer": [[37, 1, 1, "", "forward"], [37, 1, 1, "", "reset_parameters"]], "fairseq2.nn.transformer.StoreAttentionWeights": [[38, 1, 1, "", "__call__"]], "fairseq2.nn.transformer.TransformerDecoder": [[15, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerDecoderLayer": [[16, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerEncoder": [[17, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerEncoderLayer": [[18, 1, 1, "", "forward"]], "fairseq2.nn.transformer.TransformerNormOrder": [[44, 3, 1, "", "POST"], [44, 3, 1, "", "PRE"], [44, 3, 1, "", "PRE_WITH_NORMFORMER"], [44, 1, 1, "", "__iter__"]], "fairseq2.nn.utils.mask": [[45, 4, 1, "", "to_float_mask"], [46, 4, 1, "", "to_padding_mask"]], "fairseq2.optim.lr_scheduler": [[39, 0, 1, "", "CosineAnnealingLR"], [40, 0, 1, "", "LRSchedulerBase"], [41, 0, 1, "", "MyleLR"], [42, 0, 1, "", "NoamLR"], [43, 0, 1, "", "PolynomialDecayLR"]], "fairseq2.optim.lr_scheduler.CosineAnnealingLR": [[39, 1, 1, "", "get_last_lr"], [39, 1, 1, "", "load_state_dict"], [39, 1, 1, "", "print_lr"], [39, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.LRSchedulerBase": [[40, 1, 1, "", "get_last_lr"], [40, 1, 1, "", "load_state_dict"], [40, 1, 1, "", "print_lr"], [40, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.MyleLR": [[41, 1, 1, "", "get_last_lr"], [41, 1, 1, "", "load_state_dict"], [41, 1, 1, "", "print_lr"], [41, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.NoamLR": [[42, 1, 1, "", "get_last_lr"], [42, 1, 1, "", "load_state_dict"], [42, 1, 1, "", "print_lr"], [42, 1, 1, "", "state_dict"]], "fairseq2.optim.lr_scheduler.PolynomialDecayLR": [[43, 1, 1, "", "get_last_lr"], [43, 1, 1, "", "load_state_dict"], [43, 1, 1, "", "print_lr"], [43, 1, 1, "", "state_dict"]]}, "objtypes": {"0": "py:class", "1": "py:method", "2": "py:property", "3": "py:attribute", "4": "py:function"}, "objnames": {"0": ["py", "class", "Python class"], "1": ["py", "method", "Python method"], "2": ["py", "property", "Python property"], "3": ["py", "attribute", "Python attribute"], "4": ["py", "function", "Python function"]}, "titleterms": {"bibliographi": 0, "fairseq2": 1, "document": 1, "nn": 1, "refer": 1, "misc": 1, "abc": [2, 3], "protocol": [2, 3], "all": 3, "class": [3, 4], "enum": [3, 5], "function": [3, 6], "gang": 7, "positionencod": 8, "project": 9, "attentionmaskgener": 10, "attentionweighthook": 11, "feedforwardnetwork": 12, "multiheadattent": 13, "sdpa": 14, "transformerdecod": 15, "transformerdecoderlay": 16, "transformerencod": 17, "transformerencoderlay": 18, "embed": 19, "incrementalst": 20, "incrementalstatebag": 21, "learnedpositionencod": 22, "linear": 23, "modulelist": 24, "rotaryencod": 25, "sinusoidalpositionencod": 26, "tiedproject": 27, "alibiattentionmaskgener": 28, "causalattentionmaskgener": 29, "multiheadattentionst": 30, "relativepositionsdpa": 31, "standardfeedforwardnetwork": 32, "standardmultiheadattent": 33, "standardtransformerdecod": 34, "standardtransformerdecoderlay": 35, "standardtransformerencod": 36, "standardtransformerencoderlay": 37, "storeattentionweight": 38, "cosineannealinglr": 39, "lrschedulerbas": 40, "mylelr": 41, "noamlr": 42, "polynomialdecaylr": 43, "transformernormord": 44, "to_float_mask": 45, "to_padding_mask": 46}, "envversion": {"sphinx.domains.c": 2, "sphinx.domains.changeset": 1, "sphinx.domains.citation": 1, "sphinx.domains.cpp": 8, "sphinx.domains.index": 1, "sphinx.domains.javascript": 2, "sphinx.domains.math": 2, "sphinx.domains.python": 3, "sphinx.domains.rst": 2, "sphinx.domains.std": 2, "sphinx.ext.intersphinx": 1, "sphinx.ext.todo": 2, "sphinx.ext.viewcode": 1, "sphinxcontrib.bibtex": 9, "sphinx": 57}, "alltitles": {"Bibliography": [[0, "bibliography"]], "fairseq2 documentation": [[1, "fairseq2-documentation"]], "fairseq2.nn Reference": [[1, null]], "Misc": [[1, null]], "ABCs and Protocols": [[2, "abcs-and-protocols"], [3, "abcs-and-protocols"]], "All": [[3, "all"]], "Classes": [[3, "classes"], [4, "classes"]], "Enums": [[3, "enums"], [5, "enums"]], "Functions": [[3, "functions"], [6, "functions"]], "Gang": [[7, "gang"]], "PositionEncoder": [[8, "positionencoder"]], "Projection": [[9, "projection"]], "AttentionMaskGenerator": [[10, "attentionmaskgenerator"]], "AttentionWeightHook": [[11, "attentionweighthook"]], "FeedForwardNetwork": [[12, "feedforwardnetwork"]], "MultiheadAttention": [[13, "multiheadattention"]], "SDPA": [[14, "sdpa"]], "TransformerDecoder": [[15, "transformerdecoder"]], "TransformerDecoderLayer": [[16, "transformerdecoderlayer"]], "TransformerEncoder": [[17, "transformerencoder"]], "TransformerEncoderLayer": [[18, "transformerencoderlayer"]], "Embedding": [[19, "embedding"]], "IncrementalState": [[20, "incrementalstate"]], "IncrementalStateBag": [[21, "incrementalstatebag"]], "LearnedPositionEncoder": [[22, "learnedpositionencoder"]], "Linear": [[23, "linear"]], "ModuleList": [[24, "modulelist"]], "RotaryEncoder": [[25, "rotaryencoder"]], "SinusoidalPositionEncoder": [[26, "sinusoidalpositionencoder"]], "TiedProjection": [[27, "tiedprojection"]], "ALiBiAttentionMaskGenerator": [[28, "alibiattentionmaskgenerator"]], "CausalAttentionMaskGenerator": [[29, "causalattentionmaskgenerator"]], "MultiheadAttentionState": [[30, "multiheadattentionstate"]], "RelativePositionSDPA": [[31, "relativepositionsdpa"]], "StandardFeedForwardNetwork": [[32, "standardfeedforwardnetwork"]], "StandardMultiheadAttention": [[33, "standardmultiheadattention"]], "StandardTransformerDecoder": [[34, "standardtransformerdecoder"]], "StandardTransformerDecoderLayer": [[35, "standardtransformerdecoderlayer"]], "StandardTransformerEncoder": [[36, "standardtransformerencoder"]], "StandardTransformerEncoderLayer": [[37, "standardtransformerencoderlayer"]], "StoreAttentionWeights": [[38, "storeattentionweights"]], "CosineAnnealingLR": [[39, "cosineannealinglr"]], "LRSchedulerBase": [[40, "lrschedulerbase"]], "MyleLR": [[41, "mylelr"]], "NoamLR": [[42, "noamlr"]], "PolynomialDecayLR": [[43, "polynomialdecaylr"]], "TransformerNormOrder": [[44, "transformernormorder"]], "to_float_mask": [[45, "to-float-mask"]], "to_padding_mask": [[46, "to-padding-mask"]]}, "indexentries": {"gang (class in fairseq2.gang)": [[7, "fairseq2.gang.Gang"]], "all_gather() (fairseq2.gang.gang method)": [[7, "fairseq2.gang.Gang.all_gather"]], "all_reduce() (fairseq2.gang.gang method)": [[7, "fairseq2.gang.Gang.all_reduce"]], "as_process_group() (fairseq2.gang.gang method)": [[7, "fairseq2.gang.Gang.as_process_group"]], "barrier() (fairseq2.gang.gang method)": [[7, "fairseq2.gang.Gang.barrier"]], "positionencoder (class in fairseq2.nn)": [[8, "fairseq2.nn.PositionEncoder"]], "_do_forward() (fairseq2.nn.positionencoder method)": [[8, "fairseq2.nn.PositionEncoder._do_forward"]], "forward() (fairseq2.nn.positionencoder method)": [[8, "fairseq2.nn.PositionEncoder.forward"]], "projection (class in fairseq2.nn)": [[9, "fairseq2.nn.Projection"]], "forward() (fairseq2.nn.projection method)": [[9, "fairseq2.nn.Projection.forward"]], "attentionmaskgenerator (class in fairseq2.nn.transformer)": [[10, "fairseq2.nn.transformer.AttentionMaskGenerator"]], "__call__() (fairseq2.nn.transformer.attentionmaskgenerator method)": [[10, "fairseq2.nn.transformer.AttentionMaskGenerator.__call__"]], "attentionweighthook (class in fairseq2.nn.transformer)": [[11, "fairseq2.nn.transformer.AttentionWeightHook"]], "__call__() (fairseq2.nn.transformer.attentionweighthook method)": [[11, "fairseq2.nn.transformer.AttentionWeightHook.__call__"]], "feedforwardnetwork (class in fairseq2.nn.transformer)": [[12, "fairseq2.nn.transformer.FeedForwardNetwork"]], "forward() (fairseq2.nn.transformer.feedforwardnetwork method)": [[12, "fairseq2.nn.transformer.FeedForwardNetwork.forward"]], "multiheadattention (class in fairseq2.nn.transformer)": [[13, "fairseq2.nn.transformer.MultiheadAttention"]], "_run_attn_weight_hooks() (fairseq2.nn.transformer.multiheadattention method)": [[13, "fairseq2.nn.transformer.MultiheadAttention._run_attn_weight_hooks"]], "forward() (fairseq2.nn.transformer.multiheadattention method)": [[13, "fairseq2.nn.transformer.MultiheadAttention.forward"]], "register_attn_weight_hook() (fairseq2.nn.transformer.multiheadattention method)": [[13, "fairseq2.nn.transformer.MultiheadAttention.register_attn_weight_hook"]], "sdpa (class in fairseq2.nn.transformer)": [[14, "fairseq2.nn.transformer.SDPA"]], "forward() (fairseq2.nn.transformer.sdpa method)": [[14, "fairseq2.nn.transformer.SDPA.forward"]], "transformerdecoder (class in fairseq2.nn.transformer)": [[15, "fairseq2.nn.transformer.TransformerDecoder"]], "forward() (fairseq2.nn.transformer.transformerdecoder method)": [[15, "fairseq2.nn.transformer.TransformerDecoder.forward"]], "transformerdecoderlayer (class in fairseq2.nn.transformer)": [[16, "fairseq2.nn.transformer.TransformerDecoderLayer"]], "forward() (fairseq2.nn.transformer.transformerdecoderlayer method)": [[16, "fairseq2.nn.transformer.TransformerDecoderLayer.forward"]], "transformerencoder (class in fairseq2.nn.transformer)": [[17, "fairseq2.nn.transformer.TransformerEncoder"]], "forward() (fairseq2.nn.transformer.transformerencoder method)": [[17, "fairseq2.nn.transformer.TransformerEncoder.forward"]], "transformerencoderlayer (class in fairseq2.nn.transformer)": [[18, "fairseq2.nn.transformer.TransformerEncoderLayer"]], "forward() (fairseq2.nn.transformer.transformerencoderlayer method)": [[18, "fairseq2.nn.transformer.TransformerEncoderLayer.forward"]], "embedding (class in fairseq2.nn)": [[19, "fairseq2.nn.Embedding"]], "forward() (fairseq2.nn.embedding method)": [[19, "fairseq2.nn.Embedding.forward"]], "reset_parameters() (fairseq2.nn.embedding method)": [[19, "fairseq2.nn.Embedding.reset_parameters"]], "incrementalstate (class in fairseq2.nn)": [[20, "fairseq2.nn.IncrementalState"]], "reorder() (fairseq2.nn.incrementalstate method)": [[20, "fairseq2.nn.IncrementalState.reorder"]], "incrementalstatebag (class in fairseq2.nn)": [[21, "fairseq2.nn.IncrementalStateBag"]], "get_state() (fairseq2.nn.incrementalstatebag method)": [[21, "fairseq2.nn.IncrementalStateBag.get_state"]], "increment_step() (fairseq2.nn.incrementalstatebag method)": [[21, "fairseq2.nn.IncrementalStateBag.increment_step"]], "reorder() (fairseq2.nn.incrementalstatebag method)": [[21, "fairseq2.nn.IncrementalStateBag.reorder"]], "set_state() (fairseq2.nn.incrementalstatebag method)": [[21, "fairseq2.nn.IncrementalStateBag.set_state"]], "step (fairseq2.nn.incrementalstatebag property)": [[21, "fairseq2.nn.IncrementalStateBag.step"]], "learnedpositionencoder (class in fairseq2.nn)": [[22, "fairseq2.nn.LearnedPositionEncoder"]], "forward() (fairseq2.nn.learnedpositionencoder method)": [[22, "fairseq2.nn.LearnedPositionEncoder.forward"]], "reset_parameters() (fairseq2.nn.learnedpositionencoder method)": [[22, "fairseq2.nn.LearnedPositionEncoder.reset_parameters"]], "linear (class in fairseq2.nn)": [[23, "fairseq2.nn.Linear"]], "forward() (fairseq2.nn.linear method)": [[23, "fairseq2.nn.Linear.forward"]], "reset_parameters() (fairseq2.nn.linear method)": [[23, "fairseq2.nn.Linear.reset_parameters"]], "modulelist (class in fairseq2.nn)": [[24, "fairseq2.nn.ModuleList"]], "append() (fairseq2.nn.modulelist method)": [[24, "fairseq2.nn.ModuleList.append"]], "drop_iter() (fairseq2.nn.modulelist method)": [[24, "fairseq2.nn.ModuleList.drop_iter"]], "extend() (fairseq2.nn.modulelist method)": [[24, "fairseq2.nn.ModuleList.extend"]], "insert() (fairseq2.nn.modulelist method)": [[24, "fairseq2.nn.ModuleList.insert"]], "rotaryencoder (class in fairseq2.nn)": [[25, "fairseq2.nn.RotaryEncoder"]], "forward() (fairseq2.nn.rotaryencoder method)": [[25, "fairseq2.nn.RotaryEncoder.forward"]], "reset_non_persistent_buffers() (fairseq2.nn.rotaryencoder method)": [[25, "fairseq2.nn.RotaryEncoder.reset_non_persistent_buffers"]], "reset_parameters() (fairseq2.nn.rotaryencoder method)": [[25, "fairseq2.nn.RotaryEncoder.reset_parameters"]], "sinusoidalpositionencoder (class in fairseq2.nn)": [[26, "fairseq2.nn.SinusoidalPositionEncoder"]], "forward() (fairseq2.nn.sinusoidalpositionencoder method)": [[26, "fairseq2.nn.SinusoidalPositionEncoder.forward"]], "reset_non_persistent_buffers() (fairseq2.nn.sinusoidalpositionencoder method)": [[26, "fairseq2.nn.SinusoidalPositionEncoder.reset_non_persistent_buffers"]], "reset_parameters() (fairseq2.nn.sinusoidalpositionencoder method)": [[26, "fairseq2.nn.SinusoidalPositionEncoder.reset_parameters"]], "tiedprojection (class in fairseq2.nn)": [[27, "fairseq2.nn.TiedProjection"]], "forward() (fairseq2.nn.tiedprojection method)": [[27, "fairseq2.nn.TiedProjection.forward"]], "alibiattentionmaskgenerator (class in fairseq2.nn.transformer)": [[28, "fairseq2.nn.transformer.ALiBiAttentionMaskGenerator"]], "__call__() (fairseq2.nn.transformer.alibiattentionmaskgenerator method)": [[28, "fairseq2.nn.transformer.ALiBiAttentionMaskGenerator.__call__"]], "causalattentionmaskgenerator (class in fairseq2.nn.transformer)": [[29, "fairseq2.nn.transformer.CausalAttentionMaskGenerator"]], "__call__() (fairseq2.nn.transformer.causalattentionmaskgenerator method)": [[29, "fairseq2.nn.transformer.CausalAttentionMaskGenerator.__call__"]], "multiheadattentionstate (class in fairseq2.nn.transformer)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState"]], "append() (fairseq2.nn.transformer.multiheadattentionstate method)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.append"]], "cache_reserve_size (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.cache_reserve_size"]], "k (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.k"]], "key_padding_mask (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.key_padding_mask"]], "reorder() (fairseq2.nn.transformer.multiheadattentionstate method)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.reorder"]], "seq_len (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.seq_len"]], "v (fairseq2.nn.transformer.multiheadattentionstate attribute)": [[30, "fairseq2.nn.transformer.MultiheadAttentionState.v"]], "relativepositionsdpa (class in fairseq2.nn.transformer)": [[31, "fairseq2.nn.transformer.RelativePositionSDPA"]], "forward() (fairseq2.nn.transformer.relativepositionsdpa method)": [[31, "fairseq2.nn.transformer.RelativePositionSDPA.forward"]], "reset_parameters() (fairseq2.nn.transformer.relativepositionsdpa method)": [[31, "fairseq2.nn.transformer.RelativePositionSDPA.reset_parameters"]], "standardfeedforwardnetwork (class in fairseq2.nn.transformer)": [[32, "fairseq2.nn.transformer.StandardFeedForwardNetwork"]], "forward() (fairseq2.nn.transformer.standardfeedforwardnetwork method)": [[32, "fairseq2.nn.transformer.StandardFeedForwardNetwork.forward"]], "standardmultiheadattention (class in fairseq2.nn.transformer)": [[33, "fairseq2.nn.transformer.StandardMultiheadAttention"]], "_run_attn_weight_hooks() (fairseq2.nn.transformer.standardmultiheadattention method)": [[33, "fairseq2.nn.transformer.StandardMultiheadAttention._run_attn_weight_hooks"]], "forward() (fairseq2.nn.transformer.standardmultiheadattention method)": [[33, "fairseq2.nn.transformer.StandardMultiheadAttention.forward"]], "register_attn_weight_hook() (fairseq2.nn.transformer.standardmultiheadattention method)": [[33, "fairseq2.nn.transformer.StandardMultiheadAttention.register_attn_weight_hook"]], "reset_parameters() (fairseq2.nn.transformer.standardmultiheadattention method)": [[33, "fairseq2.nn.transformer.StandardMultiheadAttention.reset_parameters"]], "standardtransformerdecoder (class in fairseq2.nn.transformer)": [[34, "fairseq2.nn.transformer.StandardTransformerDecoder"]], "forward() (fairseq2.nn.transformer.standardtransformerdecoder method)": [[34, "fairseq2.nn.transformer.StandardTransformerDecoder.forward"]], "standardtransformerdecoderlayer (class in fairseq2.nn.transformer)": [[35, "fairseq2.nn.transformer.StandardTransformerDecoderLayer"]], "forward() (fairseq2.nn.transformer.standardtransformerdecoderlayer method)": [[35, "fairseq2.nn.transformer.StandardTransformerDecoderLayer.forward"]], "reset_parameters() (fairseq2.nn.transformer.standardtransformerdecoderlayer method)": [[35, "fairseq2.nn.transformer.StandardTransformerDecoderLayer.reset_parameters"]], "standardtransformerencoder (class in fairseq2.nn.transformer)": [[36, "fairseq2.nn.transformer.StandardTransformerEncoder"]], "forward() (fairseq2.nn.transformer.standardtransformerencoder method)": [[36, "fairseq2.nn.transformer.StandardTransformerEncoder.forward"]], "standardtransformerencoderlayer (class in fairseq2.nn.transformer)": [[37, "fairseq2.nn.transformer.StandardTransformerEncoderLayer"]], "forward() (fairseq2.nn.transformer.standardtransformerencoderlayer method)": [[37, "fairseq2.nn.transformer.StandardTransformerEncoderLayer.forward"]], "reset_parameters() (fairseq2.nn.transformer.standardtransformerencoderlayer method)": [[37, "fairseq2.nn.transformer.StandardTransformerEncoderLayer.reset_parameters"]], "storeattentionweights (class in fairseq2.nn.transformer)": [[38, "fairseq2.nn.transformer.StoreAttentionWeights"]], "__call__() (fairseq2.nn.transformer.storeattentionweights method)": [[38, "fairseq2.nn.transformer.StoreAttentionWeights.__call__"]], "cosineannealinglr (class in fairseq2.optim.lr_scheduler)": [[39, "fairseq2.optim.lr_scheduler.CosineAnnealingLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[39, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[39, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[39, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.cosineannealinglr method)": [[39, "fairseq2.optim.lr_scheduler.CosineAnnealingLR.state_dict"]], "lrschedulerbase (class in fairseq2.optim.lr_scheduler)": [[40, "fairseq2.optim.lr_scheduler.LRSchedulerBase"]], "get_last_lr() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[40, "fairseq2.optim.lr_scheduler.LRSchedulerBase.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[40, "fairseq2.optim.lr_scheduler.LRSchedulerBase.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[40, "fairseq2.optim.lr_scheduler.LRSchedulerBase.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.lrschedulerbase method)": [[40, "fairseq2.optim.lr_scheduler.LRSchedulerBase.state_dict"]], "mylelr (class in fairseq2.optim.lr_scheduler)": [[41, "fairseq2.optim.lr_scheduler.MyleLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.mylelr method)": [[41, "fairseq2.optim.lr_scheduler.MyleLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.mylelr method)": [[41, "fairseq2.optim.lr_scheduler.MyleLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.mylelr method)": [[41, "fairseq2.optim.lr_scheduler.MyleLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.mylelr method)": [[41, "fairseq2.optim.lr_scheduler.MyleLR.state_dict"]], "noamlr (class in fairseq2.optim.lr_scheduler)": [[42, "fairseq2.optim.lr_scheduler.NoamLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.noamlr method)": [[42, "fairseq2.optim.lr_scheduler.NoamLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.noamlr method)": [[42, "fairseq2.optim.lr_scheduler.NoamLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.noamlr method)": [[42, "fairseq2.optim.lr_scheduler.NoamLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.noamlr method)": [[42, "fairseq2.optim.lr_scheduler.NoamLR.state_dict"]], "polynomialdecaylr (class in fairseq2.optim.lr_scheduler)": [[43, "fairseq2.optim.lr_scheduler.PolynomialDecayLR"]], "get_last_lr() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[43, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.get_last_lr"]], "load_state_dict() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[43, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.load_state_dict"]], "print_lr() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[43, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.print_lr"]], "state_dict() (fairseq2.optim.lr_scheduler.polynomialdecaylr method)": [[43, "fairseq2.optim.lr_scheduler.PolynomialDecayLR.state_dict"]], "post (fairseq2.nn.transformer.transformernormorder attribute)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.POST"]], "pre (fairseq2.nn.transformer.transformernormorder attribute)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.PRE"]], "pre_with_normformer (fairseq2.nn.transformer.transformernormorder attribute)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.PRE_WITH_NORMFORMER"]], "transformernormorder (class in fairseq2.nn.transformer)": [[44, "fairseq2.nn.transformer.TransformerNormOrder"]], "__iter__() (fairseq2.nn.transformer.transformernormorder class method)": [[44, "fairseq2.nn.transformer.TransformerNormOrder.__iter__"]], "to_float_mask() (in module fairseq2.nn.utils.mask)": [[45, "fairseq2.nn.utils.mask.to_float_mask"]], "to_padding_mask() (in module fairseq2.nn.utils.mask)": [[46, "fairseq2.nn.utils.mask.to_padding_mask"]]}})